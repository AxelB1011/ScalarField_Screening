{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LV8hDCa1E78P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4XcqtkGar2G"
      },
      "outputs": [],
      "source": [
        "# Install full-stack dependencies\n",
        "!pip install -q edgartools sentence-transformers torch pandas flask sec-cik-mapper google-genai asyncpg python-dotenv langchain langchain-community\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####.."
      ],
      "metadata": {
        "id": "D0gD0coWmuH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chunking+Util+Routing"
      ],
      "metadata": {
        "id": "ZsYFJeS1FBKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from edgar import *\n",
        "import re\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import jsonpickle\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Callable, Dict, Optional\n",
        "from dateutil.parser import parse as dtparse\n",
        "from datetime import date, datetime\n",
        "from functools import lru_cache\n",
        "\n",
        "@dataclass\n",
        "class FormChunk:\n",
        "    filing_date: None\n",
        "    parent_filing_date: None\n",
        "    form_type: str\n",
        "    section_type: str\n",
        "    section_number: str\n",
        "    section_title: str\n",
        "    content: str\n",
        "    start_pos: int\n",
        "    end_pos: int\n",
        "    cik: str = \"\"\n",
        "    ticker: str = \"\"\n",
        "    fiscal_year: int = None\n",
        "    fiscal_quarter: int = None\n",
        "    chunk_id: str = \"\"\n",
        "    content_type: str = \"\"\n",
        "    char_count: int = 0\n",
        "    filing_url: str = \"\"\n",
        "    document_url: str = \"\"\n",
        "    parent_form_type: str = None\n",
        "    attachment_number: str = \"\"\n",
        "    attachment_description: str = \"\"\n",
        "    is_attachment: bool = False\n",
        "    attachment_type: str = \"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not self.char_count:\n",
        "            self.char_count = len(self.content)\n",
        "        if not self.chunk_id:\n",
        "            self._generate_chunk_id()\n",
        "\n",
        "    def _generate_chunk_id(self):\n",
        "        identifier = self.cik if self.cik else self.ticker\n",
        "        base = f\"{identifier}_{self.form_type}_{self.fiscal_year or 'UNK'}\"\n",
        "        if self.is_attachment:\n",
        "            # Use attachment number or document name for ID\n",
        "            attachment_id = self.attachment_number or hash(self.document_url) % 10000\n",
        "            self.chunk_id = f\"{base}_ATT{attachment_id}_{hash(self.content) % 10000:04d}\"\n",
        "        else:\n",
        "            self.chunk_id = f\"{base}_{self.section_number}_{hash(self.content) % 10000:04d}\"\n",
        "\n",
        "\n",
        "def _pre(text: str) -> str:\n",
        "    return text.replace(\"\\r\\n\", \"\\n\")\n",
        "\n",
        "_pat_10k = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:(PART\\s+[IVX]+)\\s*[\\.\\-]?\\s*([^\\n\\r]*?)(?:\\n|$))'\n",
        "    r'|(?:^|\\n)\\s*(?:ITEM\\s+(\\d+[A-Z]?)\\s*[\\.\\-]?\\s*([^\\n\\r]+))',\n",
        "    re.MULTILINE)\n",
        "\n",
        "def _parse_10k(m: re.Match) -> Tuple[str, str, str]:\n",
        "    if m.group(1):\n",
        "        return \"PART\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    return \"ITEM\", m.group(3), (m.group(4) or \"\").strip()\n",
        "\n",
        "_pat_10q = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:(PART\\s+[IVX]+)\\s*[\\.\\-]?\\s*([^\\n\\r]*?)(?:\\n|$))'\n",
        "    r'|(?:^|\\n)\\s*(?:ITEM\\s+(\\d+(?:\\-[A-Z])?)\\s*[\\.\\-]?\\s*([^\\n\\r]+))',\n",
        "    re.MULTILINE)\n",
        "\n",
        "def _parse_10q(m):\n",
        "    if m.group(1):\n",
        "        return \"PART\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    return \"ITEM\", m.group(3), (m.group(4) or \"\").strip()\n",
        "\n",
        "_pat_8k = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:ITEM\\s+(\\d+(?:\\.\\d+)?[A-Z]?)\\s*[\\.\\-]?\\s*([^\\n\\r]+))'\n",
        "    r'|(?:^|\\n)\\s*(SIGNATURE[S]?)\\s*$'\n",
        "    , re.MULTILINE)\n",
        "\n",
        "def _parse_8k(m):\n",
        "    if m.group(1):\n",
        "        return \"ITEM\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    return \"SIGNATURE\", \"\", m.group(3).strip()\n",
        "\n",
        "_pat_345 = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:(TABLE\\s+[IVX]+)\\s*[\\.\\-]?\\s*([^\\n\\r]*?)(?:\\n|$))'\n",
        "    r'|(?:^|\\n)\\s*(SIGNATURE[S]?)\\s*$'\n",
        "    r'|(?:^|\\n)\\s*(EXPLANATION\\s+OF\\s+RESPONSES?)\\s*$',\n",
        "    re.MULTILINE)\n",
        "\n",
        "def _parse_345(m):\n",
        "    if m.group(1):\n",
        "        return \"TABLE\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    if m.group(3):\n",
        "        return \"SIGNATURE\", \"\", m.group(3).strip()\n",
        "    return \"EXPLANATION\", \"\", m.group(4).strip()\n",
        "\n",
        "_pat_def14a = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:(PROPOSAL\\s+(?:NO\\.\\s*)?\\d+)\\s*[\\.\\-]?\\s*([^\\n\\r]+))'\n",
        "    r'|(?:^|\\n)\\s*(?:(ITEM\\s+\\d+)\\s*[\\.\\-]?\\s*([^\\n\\r]+))'\n",
        "    r'|(?:^|\\n)\\s*(EXECUTIVE\\s+COMPENSATION|CORPORATE\\s+GOVERNANCE|'\n",
        "    r'DIRECTOR\\s+NOMINEES?|SECURITY\\s+OWNERSHIP)\\s*$',\n",
        "    re.MULTILINE)\n",
        "\n",
        "def _parse_def14a(m):\n",
        "    if m.group(1):\n",
        "        return \"PROPOSAL\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    if m.group(3):\n",
        "        return \"ITEM\", m.group(3), (m.group(4) or \"\").strip()\n",
        "    return \"GOVERNANCE\", \"\", m.group(5).strip()\n",
        "\n",
        "\n",
        "_REGISTRY: Dict[str, Tuple[re.Pattern, Callable[[re.Match], Tuple[str, str, str]]]] = {\n",
        "    \"10-K\":   (_pat_10k,    _parse_10k),\n",
        "    \"10-Q\":   (_pat_10q,    _parse_10q),\n",
        "    \"8-K\":    (_pat_8k,     _parse_8k),\n",
        "    \"3\":      (_pat_345,    _parse_345),\n",
        "    \"4\":      (_pat_345,    _parse_345),\n",
        "    \"5\":      (_pat_345,    _parse_345),\n",
        "    \"DEF14A\": (_pat_def14a, _parse_def14a),\n",
        "}\n",
        "\n",
        "_pat_exhibit = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:EXHIBIT\\s+(\\d+(?:\\.\\d+)?[A-Z]?)\\s*[\\.\\-]?\\s*([^\\n\\r]*?)(?:\\n|$))'\n",
        "    r'|(?:^|\\n)\\s*(SIGNATURE[S]?)\\s*$',\n",
        "    re.MULTILINE)\n",
        "\n",
        "def _parse_exhibit(m):\n",
        "    if m.group(1):\n",
        "        return \"EXHIBIT\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    return \"SIGNATURE\", \"\", m.group(3).strip()\n",
        "\n",
        "_pat_attachment = re.compile(\n",
        "    r'(?i)(?:^|\\n)\\s*'\n",
        "    r'(?:(SCHEDULE\\s+[A-Z0-9]+)\\s*[\\.\\-]?\\s*([^\\n\\r]*?)(?:\\n|$))'\n",
        "    r'|(?:^|\\n)\\s*(?:(FORM\\s+[A-Z0-9\\-]+)\\s*[\\.\\-]?\\s*([^\\n\\r]*?)(?:\\n|$))'\n",
        "    r'|(?:^|\\n)\\s*(SIGNATURE[S]?)\\s*$',\n",
        "    re.MULTILINE)\n",
        "\n",
        "def _parse_attachment(m):\n",
        "    if m.group(1):\n",
        "        return \"SCHEDULE\", m.group(1), (m.group(2) or \"\").strip()\n",
        "    if m.group(3):\n",
        "        return \"FORM\", m.group(3), (m.group(4) or \"\").strip()\n",
        "    return \"SIGNATURE\", \"\", m.group(5).strip()\n",
        "\n",
        "_REGISTRY[\"EXHIBIT\"] = (_pat_exhibit, _parse_exhibit)\n",
        "_REGISTRY[\"ATTACHMENT\"] = (_pat_attachment, _parse_attachment)\n",
        "\n",
        "\n",
        "def chunk_form(text: str, form_type: str, **metadata) -> List[FormChunk]:\n",
        "    \"\"\"\n",
        "    Enhanced chunker with metadata support.\n",
        "\n",
        "    Args:\n",
        "        text: Document text\n",
        "        form_type: SEC form type\n",
        "        **metadata: Additional metadata (cik, ticker, filing_date, fiscal_year, etc.)\n",
        "    \"\"\"\n",
        "    form_type = form_type.upper().replace(\" \", \"\")\n",
        "    if form_type not in _REGISTRY:\n",
        "        raise ValueError(f\"Unsupported form: {form_type}\")\n",
        "\n",
        "    pattern, parser = _REGISTRY[form_type]\n",
        "    text = _pre(text)\n",
        "    matches = list(pattern.finditer(text))\n",
        "    chunks: List[FormChunk] = []\n",
        "\n",
        "    cik = metadata.get('cik', '')\n",
        "    ticker = metadata.get('ticker', '')\n",
        "    filing_date = metadata.get('filing_date', '')\n",
        "    fiscal_year = metadata.get('fiscal_year')\n",
        "    fiscal_quarter = metadata.get('fiscal_quarter')\n",
        "    filing_url = metadata.get('filing_url', '')\n",
        "    document_url = metadata.get('document_url', '')\n",
        "    is_attachment = metadata.get('is_attachment', False)\n",
        "    attachment_type = metadata.get('attachment_type', '')\n",
        "    parent_form_type = metadata.get('parent_form_type')\n",
        "    parent_filing_date = metadata.get('parent_filing_date')\n",
        "\n",
        "    if matches and matches[0].start() > 0:\n",
        "        chunks.append(\n",
        "            FormChunk(\n",
        "                form_type=form_type,\n",
        "                section_type=\"HEADER\",\n",
        "                section_number=\"\",\n",
        "                section_title=\"Document Header\",\n",
        "                content=text[:matches[0].start()],\n",
        "                start_pos=0,\n",
        "                end_pos=matches[0].start(),\n",
        "                cik=cik,\n",
        "                ticker=ticker,\n",
        "                filing_date=filing_date,\n",
        "                fiscal_year=fiscal_year,\n",
        "                fiscal_quarter=fiscal_quarter,\n",
        "                filing_url = filing_url,\n",
        "                document_url = document_url,\n",
        "                is_attachment=is_attachment,\n",
        "                attachment_type=attachment_type,\n",
        "                parent_form_type=parent_form_type,\n",
        "                parent_filing_date=parent_filing_date,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    for i, m in enumerate(matches):\n",
        "        section_type, section_number, section_title = parser(m)\n",
        "        start = m.start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "\n",
        "        content_type = _infer_content_type(section_type, section_number, section_title)\n",
        "\n",
        "        chunk = FormChunk(\n",
        "            form_type=form_type,\n",
        "            section_type=section_type,\n",
        "            section_number=section_number,\n",
        "            section_title=section_title,\n",
        "            content=text[start:end],\n",
        "            start_pos=start,\n",
        "            end_pos=end,\n",
        "            cik=cik,\n",
        "            ticker=ticker,\n",
        "            filing_date=filing_date,\n",
        "            fiscal_year=fiscal_year,\n",
        "            fiscal_quarter=fiscal_quarter,\n",
        "            content_type=content_type,\n",
        "            filing_url=filing_url,\n",
        "            document_url=document_url,\n",
        "            is_attachment=is_attachment,\n",
        "            attachment_type=attachment_type,\n",
        "            parent_form_type=parent_form_type,\n",
        "            parent_filing_date=parent_filing_date,\n",
        "        )\n",
        "\n",
        "        if is_attachment:\n",
        "            chunk.attachment_number = section_number\n",
        "            chunk.attachment_description = section_title\n",
        "\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    if not chunks:\n",
        "        chunks.append(\n",
        "            FormChunk(\n",
        "                form_type=form_type,\n",
        "                section_type=\"DOCUMENT\",\n",
        "                section_number=\"\",\n",
        "                section_title=\"Entire Document\",\n",
        "                content=text,\n",
        "                start_pos=0,\n",
        "                end_pos=len(text),\n",
        "                cik=cik,\n",
        "                ticker=ticker,\n",
        "                filing_date=filing_date,\n",
        "                fiscal_year=fiscal_year,\n",
        "                fiscal_quarter=fiscal_quarter,\n",
        "                filing_url=filing_url,\n",
        "                document_url=document_url,\n",
        "                is_attachment=is_attachment,\n",
        "                attachment_type=attachment_type,\n",
        "                parent_form_type=parent_form_type,\n",
        "                parent_filing_date=parent_filing_date,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def _infer_content_type(section_type: str, section_number: str, section_title: str) -> str:\n",
        "    \"\"\"Infer content type from section information.\"\"\"\n",
        "    title_lower = section_title.lower()\n",
        "\n",
        "    # 10-K/10-Q specific mappings\n",
        "    if section_type == \"ITEM\":\n",
        "        if \"1a\" in section_number.lower() or \"risk\" in title_lower:\n",
        "            return \"risk_factors\"\n",
        "        elif section_number == \"1\" or \"business\" in title_lower:\n",
        "            return \"business_overview\"\n",
        "        elif \"2\" in section_number or \"properties\" in title_lower:\n",
        "            return \"properties\"\n",
        "        elif \"7\" in section_number or \"md&a\" in title_lower:\n",
        "            return \"mda\"\n",
        "    elif section_type == \"EXHIBIT\":\n",
        "        return \"exhibit\"\n",
        "    elif section_type == \"SCHEDULE\":\n",
        "        return \"schedule\"\n",
        "    elif section_type == \"FORM\":\n",
        "        return \"form\"\n",
        "    elif section_type == \"SIGNATURE\":\n",
        "        return \"signature\"\n",
        "\n",
        "    return \"general\"\n",
        "\n",
        "def _determine_attachment_type(attachment) -> str:\n",
        "    \"\"\"Determine the type of attachment based on document properties.\"\"\"\n",
        "    doc_name = getattr(attachment, 'document', '').upper()\n",
        "    description = getattr(attachment, 'description', '').upper()\n",
        "\n",
        "    # Check document name patterns first (most reliable)\n",
        "    if doc_name.startswith('EX-'):\n",
        "        return \"exhibit\"\n",
        "    elif 'SCHEDULE' in doc_name:\n",
        "        return \"schedule\"\n",
        "    elif any(form in doc_name for form in ['10-', '8-K', 'DEF', 'FORM']):\n",
        "        return \"form\"\n",
        "\n",
        "    # Fallback to description if document name isn't clear\n",
        "    if 'EXHIBIT' in description:\n",
        "        return \"exhibit\"\n",
        "    elif 'SCHEDULE' in description:\n",
        "        return \"schedule\"\n",
        "\n",
        "    return \"attachment\"\n",
        "\n",
        "def process_filing_attachments(filing: object, base_metadata: Dict) -> List[FormChunk]:\n",
        "    \"\"\"\n",
        "    Process filing attachments and return chunks for all useful attachments.\n",
        "    Extracts metadata directly from the filing object.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    if not hasattr(filing, 'attachments'):\n",
        "        return all_chunks\n",
        "\n",
        "    parent_form_type = base_metadata.pop('form_type', None)\n",
        "\n",
        "    for attachment in filing.attachments:\n",
        "        # Skip non-useful attachments\n",
        "        if (attachment.purpose is None) and (attachment.extension != \".htm\"):\n",
        "            continue\n",
        "\n",
        "        # # Skip the handful of useless XML/XSD files – everything else gets parsed\n",
        "        # if attachment.extension.lower() in {\".xml\", \".xsd\", \".css\"}:\n",
        "        #     continue\n",
        "\n",
        "\n",
        "        try:\n",
        "            attachment_text = attachment.text()\n",
        "            if not attachment_text or len(attachment_text.strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            attachment_type = _determine_attachment_type(attachment)\n",
        "\n",
        "            attachment_metadata = base_metadata.copy()\n",
        "            attachment_metadata.update({\n",
        "                'is_attachment': True,\n",
        "                'attachment_type': attachment_type,\n",
        "                'parent_form_type': parent_form_type,\n",
        "                'parent_filing_date': base_metadata['filing_date'],\n",
        "                'filing_url': base_metadata['filing_url'],\n",
        "                'document_url': getattr(attachment, 'url', ''),\n",
        "                # 'attachment_description': attachment.description, #SAME AS SECTION_TITLE\n",
        "            })\n",
        "\n",
        "            chunks = [FormChunk(\n",
        "                form_type=attachment_type.upper(),\n",
        "                section_type=\"DOCUMENT\",\n",
        "                section_number=\"\",\n",
        "                section_title=getattr(attachment, 'description', 'Attachment'),\n",
        "                content=attachment_text,\n",
        "                start_pos=0,\n",
        "                end_pos=len(attachment_text),\n",
        "                **attachment_metadata\n",
        "            )]\n",
        "\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing attachment {attachment.document}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_chunks"
      ],
      "metadata": {
        "id": "u0dnTzk9as47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Util"
      ],
      "metadata": {
        "id": "D49tzVGUKGnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import jsonpickle\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Callable, Dict, Optional\n",
        "from dateutil.parser import parse as dtparse\n",
        "from datetime import date, datetime\n",
        "from functools import lru_cache\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "#  helpers_meta.py  →  robust, production-grade metadata extraction\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from sec_cik_mapper import StockMapper          # pip install sec-cik-mapper\n",
        "\n",
        "_mapper = StockMapper()\n",
        "\n",
        "\n",
        "# ---------- 1.  CIK ⇄ Ticker look-ups  (cached) -----------------------------\n",
        "@lru_cache(maxsize=4_096)\n",
        "def cik_to_ticker(cik: int) -> Optional[str]:\n",
        "    try:\n",
        "        # return _mapper.lookup_by_cik(str(cik))\n",
        "        ticketSet = _mapper.cik_to_tickers[cik.zfill(10)]\n",
        "        list_of_strings = [str(item) for item in ticketSet]\n",
        "        return list_of_strings[0]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "@lru_cache(maxsize=4_096)\n",
        "def ticker_to_cik(ticker: str) -> Optional[int]:\n",
        "    try:\n",
        "        # return int(_mapper.lookup(ticker) or 0)\n",
        "        return _mapper.ticker_to_cik[ticker]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "_RE_FY = re.compile(\n",
        "    r'for\\s+(?:the\\s+)?fiscal\\s+year\\s+ended\\s+([\\w\\s,]{5,40})',\n",
        "    re.I | re.S)\n",
        "_RE_Q  = re.compile(\n",
        "    r'for\\s+(?:the\\s+)?quarter\\s+ended\\s+([\\w\\s,]{5,40})',\n",
        "    re.I | re.S)\n",
        "\n",
        "def _extract_xbrl_fiscal_info(filing) -> Tuple[Optional[int], Optional[int]]:\n",
        "    \"\"\"\n",
        "    Extract fiscal year and quarter directly from XBRL using proper edgartools API.\n",
        "    Returns (fiscal_year, fiscal_quarter) or (None, None) if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Method 1: Use query().by_concept() - more reliable\n",
        "        doc_period_df = filing.xbrl().facts.query().by_concept(\"DocumentPeriodEndDate\").to_dataframe()\n",
        "        if not doc_period_df.empty:\n",
        "            end_date_str = doc_period_df[\"value\"].iloc[0]\n",
        "            end_date = dtparse(end_date_str).date()\n",
        "            fiscal_year = end_date.year\n",
        "\n",
        "            # Try to get DocumentFiscalPeriodFocus directly from XBRL\n",
        "            try:\n",
        "                period_focus_df = filing.xbrl().facts.query().by_concept(\"DocumentFiscalPeriodFocus\").to_dataframe()\n",
        "                if not period_focus_df.empty:\n",
        "                    period_focus = period_focus_df[\"value\"].iloc[0].strip().upper()\n",
        "\n",
        "                    # Parse fiscal period focus\n",
        "                    if period_focus == \"FY\":\n",
        "                        fiscal_quarter = None\n",
        "                    elif period_focus.startswith(\"Q\"):\n",
        "                        fiscal_quarter = int(period_focus[1])  # Q1 -> 1, Q2 -> 2, etc.\n",
        "                    else:\n",
        "                        # Fallback to calendar quarter calculation\n",
        "                        fiscal_quarter = ((end_date.month - 1) // 3 + 1) if filing.form.upper() == \"10-Q\" else None\n",
        "                else:\n",
        "                    # No DocumentFiscalPeriodFocus found, use form type logic\n",
        "                    fiscal_quarter = ((end_date.month - 1) // 3 + 1) if filing.form.upper() == \"10-Q\" else None\n",
        "\n",
        "            except Exception:\n",
        "                # Fallback to calendar quarter calculation\n",
        "                fiscal_quarter = ((end_date.month - 1) // 3 + 1) if filing.form.upper() == \"10-Q\" else None\n",
        "\n",
        "            return fiscal_year, fiscal_quarter\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        # Method 2: Use search_facts() as fallback\n",
        "        doc_period_results = filing.xbrl().facts.search_facts(\"DocumentPeriodEndDate\")\n",
        "        if doc_period_results and len(doc_period_results) > 0:\n",
        "            end_date_str = doc_period_results[0].value\n",
        "            end_date = dtparse(end_date_str).date()\n",
        "            fiscal_year = end_date.year\n",
        "\n",
        "            # Try DocumentFiscalPeriodFocus with search_facts\n",
        "            try:\n",
        "                period_focus_results = filing.xbrl().facts.search_facts(\"DocumentFiscalPeriodFocus\")\n",
        "                if period_focus_results and len(period_focus_results) > 0:\n",
        "                    period_focus = period_focus_results[0].value.strip().upper()\n",
        "\n",
        "                    if period_focus == \"FY\":\n",
        "                        fiscal_quarter = None\n",
        "                    elif period_focus.startswith(\"Q\"):\n",
        "                        fiscal_quarter = int(period_focus[1])\n",
        "                    else:\n",
        "                        fiscal_quarter = ((end_date.month - 1) // 3 + 1) if filing.form.upper() == \"10-Q\" else None\n",
        "                else:\n",
        "                    fiscal_quarter = ((end_date.month - 1) // 3 + 1) if filing.form.upper() == \"10-Q\" else None\n",
        "\n",
        "            except Exception:\n",
        "                fiscal_quarter = ((end_date.month - 1) // 3 + 1) if filing.form.upper() == \"10-Q\" else None\n",
        "\n",
        "            return fiscal_year, fiscal_quarter\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def _extract_fiscal_info(text: str, filing=None) -> Tuple[Optional[int], Optional[int]]:\n",
        "    \"\"\"\n",
        "    Robust fiscal year / quarter extraction.\n",
        "    1. XBRL DocumentPeriodEndDate + DocumentFiscalPeriodFocus (authoritative)\n",
        "    2. Cover-page regex fallback for older filings\n",
        "    Returns (fiscal_year, fiscal_quarter); quarter is None for annual filings.\n",
        "    \"\"\"\n",
        "\n",
        "    # ── 1 ─ try XBRL extraction first (most reliable) ─────────────────────\n",
        "    if filing is not None:\n",
        "        fiscal_year, fiscal_quarter = _extract_xbrl_fiscal_info(filing)\n",
        "        if fiscal_year is not None:\n",
        "            return fiscal_year, fiscal_quarter\n",
        "\n",
        "    # ── 2 ─ regex fallback on plain-text cover page ───────────────────────\n",
        "    header = \" \".join(text[:8_000].split())     # normalize whitespace\n",
        "\n",
        "    m = re.search(r'for\\s+(?:the\\s+)?fiscal\\s+year\\s+ended\\s+([\\w\\s,]{5,40})',\n",
        "                  header, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            fy = dtparse(m.group(1)).year\n",
        "            return fy, None\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    m = re.search(r'for\\s+(?:the\\s+)?quarter\\s+ended\\s+([\\w\\s,]{5,40})',\n",
        "                  header, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            end_dt = dtparse(m.group(1))\n",
        "            fy = end_dt.year\n",
        "            fq = (end_dt.month - 1) // 3 + 1\n",
        "            return fy, fq\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # ── 3 ─ give up gracefully ─────────────────────────────────────────────\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def _build_document_url(filing) -> str:\n",
        "    \"\"\"Construct direct document URL with robust error handling.\"\"\"\n",
        "    try:\n",
        "        cik = str(filing.cik)\n",
        "        accession = str(filing.accession_no).replace('-', '')\n",
        "        primary_doc = getattr(filing, 'primary_document', '')\n",
        "        is_xbrl = getattr(filing, 'is_inline_xbrl', False)\n",
        "\n",
        "        if not primary_doc:\n",
        "            return getattr(filing, 'url', '')\n",
        "\n",
        "        base_path = f\"Archives/edgar/data/{cik}/{accession}/{primary_doc}\"\n",
        "        prefix = \"ix?doc=/\" if is_xbrl else \"\"\n",
        "\n",
        "        return f\"https://www.sec.gov/{prefix}{base_path}\"\n",
        "\n",
        "    except Exception:\n",
        "        return getattr(filing, 'url', '')\n",
        "\n",
        "\n",
        "# ---------- 3.  Single entry-point ------------------------------------------\n",
        "def build_base_metadata(filing, main_text: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Construct a complete metadata dict for chunker / DB insertion.\n",
        "    Handles ALL common EDGAR forms (3,4,5, 8-K, 10-K, 10-Q, DEF14A …).\n",
        "    \"\"\"\n",
        "    fiscal_year, fiscal_quarter = _extract_fiscal_info(main_text)\n",
        "    return {\n",
        "        \"cik\":            str(filing.cik),\n",
        "        \"ticker\":         cik_to_ticker(str(filing.cik)) or '',\n",
        "        \"form_type\":      filing.form,                     # e.g. \"10-K\"\n",
        "        \"filing_date\":    str(filing.filing_date),         # YYYY-MM-DD\n",
        "        \"fiscal_year\":    fiscal_year,\n",
        "        \"fiscal_quarter\": fiscal_quarter,\n",
        "        \"filing_url\": getattr(filing, 'url', ''),           # Directory listing\n",
        "        \"document_url\": _build_document_url(filing),        # Direct document\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 2. SEC Rate Limiter (Fixes SEC API Violation)\n",
        "###############################################################################\n",
        "\n",
        "class SECRateLimiter:\n",
        "    \"\"\"Enforces SEC's 10 requests/second limit\"\"\"\n",
        "\n",
        "    def __init__(self, max_requests_per_second: int = 10):\n",
        "        self.max_rps = max_requests_per_second\n",
        "        self.requests = []\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    async def acquire(self) -> None:\n",
        "        \"\"\"Wait if necessary to respect rate limit\"\"\"\n",
        "        with self._lock:\n",
        "            now = time.time()\n",
        "\n",
        "            # Remove requests older than 1 second\n",
        "            self.requests = [req_time for req_time in self.requests if now - req_time < 1.0]\n",
        "\n",
        "            if len(self.requests) >= self.max_rps:\n",
        "                sleep_time = 1.0 - (now - self.requests[0])\n",
        "                if sleep_time > 0:\n",
        "                    await asyncio.sleep(sleep_time)\n",
        "\n",
        "            self.requests.append(now)\n",
        "\n",
        "# sec_limiter = SECRateLimiter()"
      ],
      "metadata": {
        "id": "zPnqsbNkmybV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####ROUTING"
      ],
      "metadata": {
        "id": "_-XLyN-RKHlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict, List, Set, Optional, Protocol, Tuple, Union\n",
        "from enum import Enum\n",
        "import calendar\n",
        "import re\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import jsonpickle\n",
        "from dataclasses import dataclass\n",
        "from dateutil.parser import parse as dtparse\n",
        "from datetime import date, datetime\n",
        "from functools import lru_cache\n",
        "from sec_cik_mapper import StockMapper          # pip install sec-cik-mapper\n",
        "# from utilities import *\n",
        "\n",
        "class FormType(Enum):\n",
        "    FORM_10K = \"10-K\"\n",
        "    FORM_10Q = \"10-Q\"\n",
        "    FORM_8K = \"8-K\"\n",
        "    FORM_3 = \"3\"\n",
        "    FORM_4 = \"4\"\n",
        "    FORM_5 = \"5\"\n",
        "    FORM_DEF14A = \"DEF14A\"\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class SectionIdentifier:\n",
        "    form_type: FormType\n",
        "    section_type: str  # ITEM, PART, PROPOSAL, etc.\n",
        "    section_number: str  # 1A, I, 2.02, etc.\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"{self.form_type.value} {self.section_type} {self.section_number}\".strip()\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ConceptMapping:\n",
        "    \"\"\"Maps a concept to sections across multiple forms\"\"\"\n",
        "    concept: str\n",
        "    sections: frozenset[SectionIdentifier]\n",
        "    confidence: float = 1.0\n",
        "    aliases: frozenset[str] = frozenset()\n",
        "    temporal_indicators: frozenset[str] = frozenset()  # \"quarterly\", \"annual\", \"recent\"\n",
        "\n",
        "@dataclass\n",
        "class RoutingResult:\n",
        "    \"\"\"Result of routing with form and section recommendations\"\"\"\n",
        "    primary_targets: List[SectionIdentifier]  # Best matches\n",
        "    secondary_targets: List[SectionIdentifier]  # Fallback options\n",
        "    confidence_scores: Dict[SectionIdentifier, float]\n",
        "    reasoning: str  # Human-readable explanation\n",
        "\n",
        "class UniversalConceptRepository:\n",
        "    \"\"\"Cross-form concept mappings with temporal and contextual awareness\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._concepts: Dict[str, ConceptMapping] = {}\n",
        "        self._load_universal_concepts()\n",
        "\n",
        "    def _load_universal_concepts(self) -> None:\n",
        "        \"\"\"Load comprehensive cross-form concept mappings\"\"\"\n",
        "\n",
        "        mappings = [\n",
        "            # ===== CORE BUSINESS & STRATEGY =====\n",
        "            ConceptMapping(\n",
        "                \"business_overview\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                ]),\n",
        "                confidence=0.90,\n",
        "                aliases=frozenset([\"business_description\", \"company_overview\", \"operations\", \"what_does_company_do\",\n",
        "                                 \"business_model\", \"corporate_overview\", \"business_activities\", \"primary_business\"]),\n",
        "                temporal_indicators=frozenset([\"annual\", \"detailed\", \"comprehensive\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"competitive_positioning\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1A\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                ]),\n",
        "                confidence=0.83,\n",
        "                aliases=frozenset([\"competitive_advantages\", \"competitive_advantage\", \"market_position\", \"competitive_strategy\",\n",
        "                                 \"differentiation\", \"competitive_landscape\", \"market_share\", \"competitive_strengths\",\n",
        "                                 \"strategic_positioning\", \"moat\", \"strategic_advantages\"]),\n",
        "                temporal_indicators=frozenset([\"strategic\", \"current\", \"annual\"])\n",
        "            ),\n",
        "\n",
        "            # ===== CAPITAL STRUCTURE & FINANCING =====\n",
        "            ConceptMapping(\n",
        "                \"debt_and_credit_facilities\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"1.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"2.03\"),\n",
        "                ]),\n",
        "                confidence=0.90,\n",
        "                aliases=frozenset([\"debt\", \"credit_facilities\", \"borrowings\", \"loans\", \"credit_agreements\",\n",
        "                                 \"revolving_credit\", \"term_loans\", \"bonds\", \"notes\", \"debt_covenants\",\n",
        "                                 \"credit_rating\", \"debt_maturity\", \"interest_expense\"]),\n",
        "                temporal_indicators=frozenset([\"quarterly\", \"annual\", \"recent\", \"new\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"capital_allocation\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"2.02\"),\n",
        "                ]),\n",
        "                confidence=0.88,\n",
        "                aliases=frozenset([\"dividends\", \"share_buybacks\", \"capital_investments\", \"acquisitions\",\n",
        "                                 \"capex\", \"capital_expenditures\", \"cash_deployment\", \"shareholder_returns\",\n",
        "                                 \"investment_strategy\", \"capital_priorities\"]),\n",
        "                temporal_indicators=frozenset([\"annual\", \"quarterly\", \"strategic\", \"recent\"])\n",
        "            ),\n",
        "\n",
        "            # ===== FINANCIAL PERFORMANCE & METRICS =====\n",
        "            ConceptMapping(\n",
        "                \"financial_performance\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"2.02\"),\n",
        "                ]),\n",
        "                confidence=0.95,\n",
        "                aliases=frozenset([\"revenue\", \"earnings\", \"profitability\", \"financial_results\", \"operating_performance\",\n",
        "                                 \"financial_metrics\", \"key_metrics\", \"performance_indicators\", \"financial_condition\",\n",
        "                                 \"revenue_drivers\", \"revenue_sources\", \"revenue_segments\", \"primary_revenue\",\n",
        "                                 \"sales\", \"income\", \"profit\"]),\n",
        "                temporal_indicators=frozenset([\"quarterly\", \"annual\", \"recent\", \"latest\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"cash_flow_and_liquidity\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),\n",
        "                ]),\n",
        "                confidence=0.88,\n",
        "                aliases=frozenset([\"cash_flow\", \"operating_cash_flow\", \"free_cash_flow\", \"liquidity\", \"cash_position\",\n",
        "                                 \"working_capital\", \"cash_management\", \"liquidity_management\", \"cash_generation\",\n",
        "                                 \"working_capital_changes\", \"current_assets\", \"current_liabilities\"]),\n",
        "                temporal_indicators=frozenset([\"quarterly\", \"annual\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"segment_performance\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),\n",
        "                ]),\n",
        "                confidence=0.92,\n",
        "                aliases=frozenset([\"business_segments\", \"operating_segments\", \"geographic_segments\",\n",
        "                                 \"product_lines\", \"divisional_performance\", \"segment_revenue\",\n",
        "                                 \"segment_margins\", \"segment_profitability\"]),\n",
        "                temporal_indicators=frozenset([\"quarterly\", \"annual\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"cost_structure\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),\n",
        "                ]),\n",
        "                confidence=0.85,\n",
        "                aliases=frozenset([\"cost_of_goods_sold\", \"operating_expenses\", \"sg&a\", \"cost_reduction\",\n",
        "                                 \"efficiency_initiatives\", \"margin_improvement\", \"cost_control\",\n",
        "                                 \"operating_leverage\"]),\n",
        "                temporal_indicators=frozenset([\"quarterly\", \"annual\"])\n",
        "            ),\n",
        "\n",
        "            # ===== RISK MANAGEMENT & COMPLIANCE =====\n",
        "            ConceptMapping(\n",
        "                \"risk_factors\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1A\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1A\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                ]),\n",
        "                confidence=0.95,\n",
        "                aliases=frozenset([\"risks\", \"uncertainties\", \"threats\", \"challenges\", \"risk_management\", \"risk_factors\",\n",
        "                                 \"business_risks\", \"operational_risks\", \"market_risks\", \"credit_risks\", \"regulatory_risks\",\n",
        "                                 \"cybersecurity_risks\", \"supply_chain_risks\", \"risk_assessment\"]),\n",
        "                temporal_indicators=frozenset([\"annual\", \"recent\", \"emerging\", \"yearly\", \"comprehensive\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"regulatory_and_legal\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"3\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"PART II\", \"ITEM 1\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                ]),\n",
        "                confidence=0.87,\n",
        "                aliases=frozenset([\"legal_proceedings\", \"litigation\", \"regulatory_compliance\", \"government_regulation\",\n",
        "                                 \"lawsuits\", \"settlements\", \"regulatory_changes\", \"compliance_costs\", \"regulatory_actions\",\n",
        "                                 \"legal_issues\", \"court_cases\", \"legal_matters\"]),\n",
        "                temporal_indicators=frozenset([\"ongoing\", \"recent\", \"current\"])\n",
        "            ),\n",
        "\n",
        "            # ===== GOVERNANCE & EXECUTIVE MANAGEMENT =====\n",
        "            ConceptMapping(\n",
        "                \"executive_compensation\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"ITEM\", \"11\"),\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"ITEM\", \"12\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"11\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"5.02\"),\n",
        "                ]),\n",
        "                confidence=0.95,\n",
        "                aliases=frozenset([\"executive_pay\", \"ceo_compensation\", \"management_compensation\", \"salary\", \"bonus\",\n",
        "                                 \"stock_options\", \"equity_awards\", \"incentive_plans\", \"compensation_philosophy\",\n",
        "                                 \"pay_for_performance\", \"ceo_pay\", \"executive_salary\", \"compensation\"]),\n",
        "                temporal_indicators=frozenset([\"annual\", \"yearly\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"corporate_governance\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"ITEM\", \"10\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"10\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"5.02\"),\n",
        "                ]),\n",
        "                confidence=0.90,\n",
        "                aliases=frozenset([\"board_of_directors\", \"corporate_governance\", \"board_independence\", \"board_committees\",\n",
        "                                 \"director_qualifications\", \"governance_policies\", \"board_oversight\", \"shareholder_rights\",\n",
        "                                 \"board\", \"directors\", \"governance\", \"board_composition\", \"independent_directors\"]),\n",
        "                temporal_indicators=frozenset([\"annual\", \"current\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"ownership_structure\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"ITEM\", \"12\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"12\"),\n",
        "                    SectionIdentifier(FormType.FORM_3, \"INITIAL\", \"OWNERSHIP\"),\n",
        "                ]),\n",
        "                confidence=0.85,\n",
        "                aliases=frozenset([\"beneficial_ownership\", \"major_shareholders\", \"shareholding\", \"ownership\",\n",
        "                                 \"equity_ownership\", \"stock_ownership\", \"shareholder_structure\", \"ownership_table\"]),\n",
        "                temporal_indicators=frozenset([\"current\", \"as_of\"])\n",
        "            ),\n",
        "\n",
        "            # ===== INSIDER ACTIVITY =====\n",
        "            ConceptMapping(\n",
        "                \"insider_trading\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_4, \"OWNERSHIP\", \"NON_DERIVATIVE\"),\n",
        "                    SectionIdentifier(FormType.FORM_4, \"OWNERSHIP\", \"DERIVATIVE\"),\n",
        "                    SectionIdentifier(FormType.FORM_5, \"ANNUAL\", \"SUMMARY\"),\n",
        "                    SectionIdentifier(FormType.FORM_3, \"INITIAL\", \"OWNERSHIP\"),\n",
        "                ]),\n",
        "                confidence=0.90,\n",
        "                aliases=frozenset([\"insider_transactions\", \"insider_buying\", \"insider_selling\", \"officer_trading\",\n",
        "                                 \"director_trading\", \"executive_trading\", \"stock_transactions\", \"insider_activity\",\n",
        "                                 \"share_purchases\", \"share_sales\", \"stock_sales\", \"stock_purchases\"]),\n",
        "                temporal_indicators=frozenset([\"recent\", \"current\", \"latest\"])\n",
        "            ),\n",
        "\n",
        "            # ===== STRATEGIC INITIATIVES & INNOVATION =====\n",
        "            ConceptMapping(\n",
        "                \"mergers_acquisitions\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"1.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"2.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                ]),\n",
        "                confidence=0.92,\n",
        "                aliases=frozenset([\"mergers\", \"acquisitions\", \"divestitures\", \"asset_sales\", \"strategic_transactions\",\n",
        "                                 \"business_combinations\", \"joint_ventures\", \"strategic_partnerships\", \"M&A\", \"deal_rationale\",\n",
        "                                 \"merger\", \"acquisition\", \"takeover\", \"asset_purchase\", \"divestiture\", \"spin_off\",\n",
        "                                 \"deal\", \"transaction\", \"strategic_rationale\", \"business_combination\"]),\n",
        "                temporal_indicators=frozenset([\"recent\", \"announced\", \"completed\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"research_development\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),    # Business overview mentions R&D\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),    # MD&A discusses R&D spending\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"8\"),    # Financial statements show R&D expenses\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"1\"),    # Quarterly financials\n",
        "                    SectionIdentifier(FormType.FORM_10Q, \"ITEM\", \"2\"),    # Quarterly MD&A\n",
        "                ]),\n",
        "                confidence=0.85,\n",
        "                aliases=frozenset([\n",
        "                    \"r&d\", \"R&D\", \"research\", \"development\", \"r_d\", \"research_and_development\", \"technology_development\",\n",
        "                    \"rd_spending\", \"research_costs\", \"development_costs\", \"innovation\", \"new_product_development\", \"innovation_investment\",\n",
        "                    \"research_expenses\", \"development_expenses\", \"rd_investment\", \"R&D_spending\", \"research_spending\",\n",
        "                ]),\n",
        "                temporal_indicators=frozenset([\"spending\", \"costs\", \"expenses\", \"investment\", \"annual\", \"quarterly\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"technology_and_innovation\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                ]),\n",
        "                confidence=0.85,\n",
        "                aliases=frozenset([\"technology\", \"digital_transformation\", \"AI\", \"artificial_intelligence\", \"automation\",\n",
        "                                 \"innovation_strategy\", \"technology_platform\", \"digital_capabilities\", \"tech_infrastructure\",\n",
        "                                 \"machine_learning\", \"automation_strategies\", \"digital_initiatives\", \"tech_positioning\"]),\n",
        "                temporal_indicators=frozenset([\"strategic\", \"emerging\", \"recent\"])\n",
        "            ),\n",
        "\n",
        "            # ===== ESG & SUSTAINABILITY =====\n",
        "            ConceptMapping(\n",
        "                \"esg_and_sustainability\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1A\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"7\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                ]),\n",
        "                confidence=0.80,\n",
        "                aliases=frozenset([\"ESG\", \"sustainability\", \"environmental\", \"social_responsibility\", \"climate_change\",\n",
        "                                 \"carbon_emissions\", \"diversity_inclusion\", \"corporate_responsibility\", \"sustainable_practices\",\n",
        "                                 \"climate\", \"climate_related_risks\", \"environmental_risks\", \"carbon_footprint\"]),\n",
        "                temporal_indicators=frozenset([\"annual\", \"strategic\", \"emerging\"])\n",
        "            ),\n",
        "\n",
        "            # ===== CORPORATE CHANGES & EVENTS =====\n",
        "            ConceptMapping(\n",
        "                \"material_agreements\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"1.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_10K, \"ITEM\", \"1\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"8.01\"),\n",
        "                ]),\n",
        "                confidence=0.90,\n",
        "                aliases=frozenset([\"contracts\", \"agreements\", \"partnerships\", \"deals\", \"material_contracts\",\n",
        "                                 \"strategic_agreements\", \"joint_ventures\", \"alliances\", \"licensing_agreements\"]),\n",
        "                temporal_indicators=frozenset([\"recent\", \"new\", \"latest\"])\n",
        "            ),\n",
        "\n",
        "            ConceptMapping(\n",
        "                \"management_changes\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"5.02\"),\n",
        "                    SectionIdentifier(FormType.FORM_8K, \"ITEM\", \"5.01\"),\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"ITEM\", \"1\"),\n",
        "                ]),\n",
        "                confidence=0.85,\n",
        "                aliases=frozenset([\"executive_changes\", \"ceo_change\", \"management_turnover\", \"leadership_changes\",\n",
        "                                 \"officer_changes\", \"director_changes\", \"management_succession\", \"executive_appointments\"]),\n",
        "                temporal_indicators=frozenset([\"recent\", \"new\", \"latest\"])\n",
        "            ),\n",
        "\n",
        "            # ===== SHAREHOLDER MATTERS =====\n",
        "            ConceptMapping(\n",
        "                \"shareholder_proposals\",\n",
        "                frozenset([\n",
        "                    SectionIdentifier(FormType.FORM_DEF14A, \"PROPOSAL\", \"\"),\n",
        "                ]),\n",
        "                confidence=0.95,\n",
        "                aliases=frozenset([\"proposals\", \"voting_matters\", \"proxy_voting\", \"shareholder_votes\",\n",
        "                                 \"shareholder_proposals\", \"ballot_items\", \"voting_items\"]),\n",
        "                temporal_indicators=frozenset([\"upcoming\", \"annual\"])\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "\n",
        "        for mapping in mappings:\n",
        "            self._concepts[mapping.concept] = mapping\n",
        "            # Also add aliases as separate entries for easier lookup\n",
        "            for alias in mapping.aliases:\n",
        "                if alias not in self._concepts:  # Don't overwrite existing primary concepts\n",
        "                    alias_mapping = ConceptMapping(\n",
        "                        alias, mapping.sections, mapping.confidence * 0.9,\n",
        "                        frozenset(), mapping.temporal_indicators\n",
        "                    )\n",
        "                    self._concepts[alias] = alias_mapping\n",
        "\n",
        "    def get_concept(self, concept_name: str) -> Optional[ConceptMapping]:\n",
        "        return self._concepts.get(concept_name)\n",
        "\n",
        "    def get_all_concepts(self) -> Dict[str, ConceptMapping]:\n",
        "        return self._concepts.copy()\n",
        "\n",
        "\n",
        "class TemporalMatcher:\n",
        "    \"\"\"Matches temporal indicators to determine form preferences\"\"\"\n",
        "\n",
        "    TEMPORAL_FORM_PREFERENCES = {\n",
        "        # Recent/current events favor 8-K and most recent periodic reports\n",
        "        frozenset([\"recent\", \"latest\", \"current\", \"new\", \"announced\"]): [\n",
        "            FormType.FORM_8K, FormType.FORM_10Q, FormType.FORM_4\n",
        "        ],\n",
        "\n",
        "        # Quarterly indicators\n",
        "        frozenset([\"quarterly\", \"quarter\", \"q1\", \"q2\", \"q3\", \"q4\"]): [\n",
        "            FormType.FORM_10Q, FormType.FORM_8K\n",
        "        ],\n",
        "\n",
        "        # Annual indicators\n",
        "        frozenset([\"annual\", \"yearly\", \"comprehensive\", \"detailed\"]): [\n",
        "            FormType.FORM_10K, FormType.FORM_DEF14A, FormType.FORM_5\n",
        "        ],\n",
        "\n",
        "        # Voting/governance timing\n",
        "        frozenset([\"upcoming\", \"proposed\", \"proxy\"]): [\n",
        "            FormType.FORM_DEF14A\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    def get_temporal_preferences(self, query: str) -> List[FormType]:\n",
        "        \"\"\"Return form types preferred based on temporal indicators in query\"\"\"\n",
        "        query_lower = query.lower()\n",
        "        preferences = []\n",
        "\n",
        "        for indicators, forms in self.TEMPORAL_FORM_PREFERENCES.items():\n",
        "            if any(indicator in query_lower for indicator in indicators):\n",
        "                preferences.extend(forms)\n",
        "\n",
        "        return preferences\n",
        "\n",
        "class CrossFormRouter:\n",
        "    \"\"\"Main routing service that determines both form and section\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.repository = UniversalConceptRepository()\n",
        "        self.temporal_matcher = TemporalMatcher()\n",
        "\n",
        "        # Enhanced synonym matching\n",
        "        self.fuzzy_synonyms = {\n",
        "            \"compensation\": [\"pay\", \"salary\", \"remuneration\", \"earnings\", \"income\"],\n",
        "            \"risk\": [\"danger\", \"threat\", \"uncertainty\", \"hazard\", \"exposure\"],\n",
        "            \"performance\": [\"results\", \"outcomes\", \"success\", \"achievement\"],\n",
        "            \"ownership\": [\"shareholding\", \"stake\", \"holdings\", \"equity\"],\n",
        "            \"legal\": [\"court\", \"lawsuit\", \"litigation\", \"judicial\"],\n",
        "            \"management\": [\"leadership\", \"executives\", \"administration\"],\n",
        "            \"research\": [\"R&D\", \"development\", \"innovation\", \"technology\"],\n",
        "            \"capital\": [\"liquidity\", \"cash\", \"working capital\", \"funds\"],\n",
        "            \"artificial_intelligence\": [\"AI\", \"automation\", \"machine learning\", \"digital\"],\n",
        "            \"competitive\": [\"advantage\", \"positioning\", \"differentiation\", \"strategy\"]\n",
        "        }\n",
        "\n",
        "    @lru_cache(maxsize=1000)\n",
        "    def route_question(self, question: str, confidence_threshold: float = 0.3) -> RoutingResult:\n",
        "        \"\"\"\n",
        "        Route a natural language question to specific form sections\n",
        "\n",
        "        Args:\n",
        "            question: User's natural language question\n",
        "            confidence_threshold: Minimum confidence for inclusion\n",
        "\n",
        "        Returns:\n",
        "            RoutingResult with prioritized sections and reasoning\n",
        "        \"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Get temporal preferences\n",
        "        temporal_preferences = self.temporal_matcher.get_temporal_preferences(question)\n",
        "\n",
        "        # Find matching concepts\n",
        "        concept_matches = self._find_concept_matches(question_lower)\n",
        "\n",
        "        # Score and rank sections\n",
        "        section_scores = self._calculate_section_scores(\n",
        "            concept_matches, temporal_preferences, question_lower\n",
        "        )\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        qualified_sections = {\n",
        "            section: score for section, score in section_scores.items()\n",
        "            if score >= confidence_threshold\n",
        "        }\n",
        "\n",
        "        # Sort by score\n",
        "        sorted_sections = sorted(qualified_sections.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Split into primary and secondary targets\n",
        "        primary_targets = [section for section, score in sorted_sections[:3]]\n",
        "        secondary_targets = [section for section, score in sorted_sections[3:6]]\n",
        "\n",
        "        # Generate reasoning\n",
        "        reasoning = self._generate_reasoning(concept_matches, temporal_preferences, primary_targets)\n",
        "\n",
        "        return RoutingResult(\n",
        "            primary_targets=primary_targets,\n",
        "            secondary_targets=secondary_targets,\n",
        "            confidence_scores=dict(sorted_sections),\n",
        "            reasoning=reasoning\n",
        "        )\n",
        "\n",
        "    def _find_concept_matches(self, query_lower: str) -> List[Tuple[str, ConceptMapping, float]]:\n",
        "        \"\"\"Find concepts that match the query with confidence scores\"\"\"\n",
        "        matches = []\n",
        "\n",
        "        for concept_name, mapping in self.repository.get_all_concepts().items():\n",
        "            confidence = 0.0\n",
        "\n",
        "            # Direct concept name match\n",
        "            if concept_name.replace(\"_\", \" \") in query_lower:\n",
        "                confidence = 1.0\n",
        "\n",
        "            # Alias matching\n",
        "            elif any(alias.replace(\"_\", \" \") in query_lower for alias in mapping.aliases):\n",
        "                confidence = 0.9\n",
        "\n",
        "            # Fuzzy synonym matching\n",
        "            elif concept_name in self.fuzzy_synonyms:\n",
        "                synonym_matches = sum(1 for syn in self.fuzzy_synonyms[concept_name]\n",
        "                                    if syn in query_lower)\n",
        "                if synonym_matches > 0:\n",
        "                    confidence = 0.7 * (synonym_matches / len(self.fuzzy_synonyms[concept_name]))\n",
        "\n",
        "            if confidence > 0:\n",
        "                matches.append((concept_name, mapping, confidence))\n",
        "\n",
        "        return matches\n",
        "\n",
        "    def _calculate_section_scores(self, concept_matches: List[Tuple[str, ConceptMapping, float]],\n",
        "                                temporal_preferences: List[FormType], query: str) -> Dict[SectionIdentifier, float]:\n",
        "        \"\"\"Calculate final scores for each section considering all factors\"\"\"\n",
        "        section_scores = {}\n",
        "\n",
        "        for concept_name, mapping, match_confidence in concept_matches:\n",
        "            base_score = match_confidence * mapping.confidence\n",
        "\n",
        "            for section in mapping.sections:\n",
        "                current_score = section_scores.get(section, 0.0)\n",
        "\n",
        "                # Apply temporal boost\n",
        "                temporal_boost = 1.2 if section.form_type in temporal_preferences else 1.0\n",
        "\n",
        "                # Apply query-specific boosts\n",
        "                query_boost = self._get_query_specific_boost(section, query)\n",
        "\n",
        "                final_score = base_score * temporal_boost * query_boost\n",
        "                section_scores[section] = max(current_score, final_score)\n",
        "\n",
        "        return section_scores\n",
        "\n",
        "    def _get_query_specific_boost(self, section: SectionIdentifier, query: str) -> float:\n",
        "        \"\"\"Apply boosts based on query-specific indicators\"\"\"\n",
        "        boost = 1.0\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Question type boosts\n",
        "        if any(word in query_lower for word in [\"how much\", \"what did\", \"amount\"]):\n",
        "            if \"compensation\" in query_lower and section.form_type == FormType.FORM_DEF14A:\n",
        "                boost *= 1.3\n",
        "\n",
        "        if any(word in query_lower for word in [\"recent\", \"latest\", \"new\"]):\n",
        "            if section.form_type in [FormType.FORM_8K, FormType.FORM_4]:\n",
        "                boost *= 1.3\n",
        "\n",
        "        if \"annual\" in query_lower:\n",
        "            if section.form_type in [FormType.FORM_10K, FormType.FORM_DEF14A]:\n",
        "                boost *= 1.2\n",
        "\n",
        "        return boost\n",
        "\n",
        "    def _generate_reasoning(self, concept_matches: List[Tuple[str, ConceptMapping, float]],\n",
        "                          temporal_preferences: List[FormType],\n",
        "                          primary_targets: List[SectionIdentifier]) -> str:\n",
        "        \"\"\"Generate human-readable explanation of routing decisions\"\"\"\n",
        "        if not primary_targets:\n",
        "            return \"No strong matches found for the query.\"\n",
        "\n",
        "        reasoning_parts = []\n",
        "\n",
        "        # Primary concept identified\n",
        "        if concept_matches:\n",
        "            top_concept = concept_matches[0][0].replace(\"_\", \" \").title()\n",
        "            reasoning_parts.append(f\"Identified primary concept: {top_concept}\")\n",
        "\n",
        "        # Temporal reasoning\n",
        "        if temporal_preferences:\n",
        "            forms = [f.value for f in temporal_preferences[:2]]\n",
        "            reasoning_parts.append(f\"Query suggests focus on: {', '.join(forms)}\")\n",
        "\n",
        "        # Section recommendations\n",
        "        if primary_targets:\n",
        "            sections = [str(target) for target in primary_targets[:2]]\n",
        "            reasoning_parts.append(f\"Best matches: {', '.join(sections)}\")\n",
        "\n",
        "        return \". \".join(reasoning_parts)\n",
        "\n",
        "\n",
        "def route_user_question(question: str, available_chunks: List['FormChunk']) -> Tuple[List['FormChunk'], str]:\n",
        "    \"\"\"\n",
        "    Main integration function - routes user question to relevant chunks across all forms\n",
        "\n",
        "    Args:\n",
        "        question: User's natural language question\n",
        "        available_chunks: All chunks from all forms for the company\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (relevant_chunks, explanation)\n",
        "    \"\"\"\n",
        "    router = CrossFormRouter()\n",
        "    routing_result = router.route_question(question)\n",
        "\n",
        "    if not routing_result.primary_targets:\n",
        "        # Fallback to content-length based filtering\n",
        "        fallback_chunks = [chunk for chunk in available_chunks if len(chunk.content) > 1000]\n",
        "        return fallback_chunks[:5], \"No specific sections identified - returning substantial content chunks\"\n",
        "\n",
        "    relevant_chunks = []\n",
        "\n",
        "    # First priority: exact matches\n",
        "    for target in routing_result.primary_targets:\n",
        "        for chunk in available_chunks:\n",
        "            if (chunk.form_type == target.form_type.value and\n",
        "                chunk.section_type == target.section_type and\n",
        "                target.section_number in chunk.section_number):\n",
        "                relevant_chunks.append(chunk)\n",
        "\n",
        "    # Second priority: partial matches if we don't have enough\n",
        "    if len(relevant_chunks) < 3:\n",
        "        for target in routing_result.secondary_targets:\n",
        "            for chunk in available_chunks:\n",
        "                if (chunk.form_type == target.form_type.value and\n",
        "                    chunk.section_type == target.section_type):\n",
        "                    if chunk not in relevant_chunks:\n",
        "                        relevant_chunks.append(chunk)\n",
        "\n",
        "    return relevant_chunks, routing_result.reasoning\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TickerContext:\n",
        "    \"\"\"Ticker/entity identification\"\"\"\n",
        "    tickers: frozenset[str]  # {\"AAPL\", \"MSFT\"}\n",
        "    comparison_mode: bool  # True if comparing multiple entities\n",
        "\n",
        "class TemporalScope(Enum):\n",
        "    SPECIFIC_YEAR = \"specific_year\"      # \"2022\", \"2021\"\n",
        "    SPECIFIC_QUARTER = \"specific_quarter\" # \"Q1 2023\"\n",
        "    RECENT = \"recent\"                    # \"recent\", \"latest\"\n",
        "    HISTORICAL_TREND = \"historical\"      # \"over time\", \"trend\"\n",
        "    ANNUAL = \"annual\"                    # \"annual\", \"yearly\"\n",
        "    QUARTERLY = \"quarterly\"              # \"quarterly\"\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TemporalContext:\n",
        "    \"\"\"Temporal scope and preferences\"\"\"\n",
        "    scope: TemporalScope\n",
        "    specific_year: Optional[int] = None\n",
        "    specific_quarter: Optional[int] = None  # 1, 2, 3, 4\n",
        "    year_range: Optional[Tuple[int, int]] = None  # (start_year, end_year)\n",
        "\n",
        "class DocumentScope(Enum):\n",
        "    ANY = \"any\"\n",
        "    FORM_10K = \"10k\"\n",
        "    FORM_10Q = \"10q\"\n",
        "    FORM_8K = \"8k\"\n",
        "    FORM_DEF14A = \"def14a\"\n",
        "    FORM_3 = \"3\"\n",
        "    FORM_4 = \"4\"\n",
        "    FORM_5 = \"5\"\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DocumentContext:\n",
        "    \"\"\"Document type specification\"\"\"\n",
        "    scope: DocumentScope\n",
        "    must_include: frozenset[FormType] = frozenset()\n",
        "    exclude: frozenset[FormType] = frozenset()\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class QueryContext:\n",
        "    \"\"\"Complete query context\"\"\"\n",
        "    ticker_context: TickerContext\n",
        "    temporal_context: TemporalContext\n",
        "    document_context: DocumentContext\n",
        "    original_query: str\n",
        "\n",
        "class TickerExtractor:\n",
        "    \"\"\"Fixed ticker extraction with proper validation\"\"\"\n",
        "\n",
        "    # Comprehensive company name to ticker mapping\n",
        "    COMPANY_NAME_MAP = {\n",
        "        # Major tech companies\n",
        "        r'\\b(?:apple|apple\\s+inc\\.?)\\b': 'AAPL',\n",
        "        r'\\b(?:microsoft|microsoft\\s+corp\\.?)\\b': 'MSFT',\n",
        "        r'\\b(?:google|alphabet|alphabet\\s+inc\\.?)\\b': 'GOOGL',\n",
        "        r'\\b(?:amazon|amazon\\.com)\\b': 'AMZN',\n",
        "        r'\\b(?:tesla|tesla\\s+inc\\.?)\\b': 'TSLA',\n",
        "        r'\\b(?:meta|facebook|meta\\s+platforms)\\b': 'META',\n",
        "        r'\\b(?:nvidia|nvidia\\s+corp\\.?)\\b': 'NVDA',\n",
        "        r'\\b(?:netflix|netflix\\s+inc\\.?)\\b': 'NFLX',\n",
        "\n",
        "        # Financial companies\n",
        "        r'\\b(?:jpmorgan|jp\\s+morgan|jpm)\\b': 'JPM',\n",
        "        r'\\b(?:bank\\s+of\\s+america|bofa)\\b': 'BAC',\n",
        "        r'\\b(?:wells\\s+fargo)\\b': 'WFC',\n",
        "        r'\\b(?:goldman\\s+sachs)\\b': 'GS',\n",
        "\n",
        "        # Other major companies\n",
        "        r'\\b(?:berkshire\\s+hathaway)\\b': 'BRK-A',\n",
        "        r'\\b(?:johnson\\s+&\\s+johnson|j&j)\\b': 'JNJ',\n",
        "        r'\\b(?:procter\\s+&\\s+gamble|p&g)\\b': 'PG',\n",
        "        r'\\b(?:coca[\\-\\s]?cola)\\b': 'KO',\n",
        "    }\n",
        "\n",
        "    _mapper = StockMapper()\n",
        "    # Known ticker symbols\n",
        "    VALID_TICKERS = _mapper.ticker_to_cik.keys()\n",
        "\n",
        "    # Words that are definitely NOT tickers (common false positives)\n",
        "    EXCLUDED_WORDS = {\n",
        "        'WHAT', 'ARE', 'THE', 'AND', 'FOR', 'HAS', 'HOW', 'OVER', 'TIME',\n",
        "        'AT', 'IN', 'ON', 'TO', 'OF', 'IS', 'WAS', 'BEEN', 'HAVE', 'HAD', 'R&D',\n",
        "        'WILL', 'CAN', 'MAY', 'SHOULD', 'COULD', 'WOULD', 'RISK', 'RISKS', 'M&A',\n",
        "        'FROM', 'WITH', 'BY', 'AS', 'AN', 'A', 'THIS', 'THAT', 'THESE', 'THOSE'\n",
        "    }\n",
        "\n",
        "    COMPARISON_INDICATORS = [\n",
        "        r'\\bcompare\\b', r'\\bcomparison\\b', r'\\bvs\\.?\\b', r'\\bversus\\b',\n",
        "        r'\\bagainst\\b', r'\\brelative\\s+to\\b', r'\\bcompared\\s+to\\b',\n",
        "        r'\\bdifference\\s+between\\b', r'\\bboth\\s+\\w+\\s+and\\b', r'\\beither\\s+\\w+\\s+or\\b'\n",
        "    ]\n",
        "\n",
        "    def extract(self, query: str) -> TickerContext:\n",
        "        \"\"\"Extract ticker context with proper validation\"\"\"\n",
        "        found_tickers = set()\n",
        "\n",
        "        # 1. Extract by company name patterns\n",
        "        query_lower = query.lower()\n",
        "        for pattern, ticker in self.COMPANY_NAME_MAP.items():\n",
        "            if re.search(pattern, query_lower):\n",
        "                found_tickers.add(ticker)\n",
        "\n",
        "        # 2. Extract explicit ticker mentions (more restrictive)\n",
        "        # Look for patterns like \"AAPL's\" or \"(AAPL)\" or \"AAPL stock\"\n",
        "        ticker_patterns = [\n",
        "            r'\\b([A-Z]{2,5})[\\'\\s]s\\b',      # AAPL's, MSFT's\n",
        "            r'\\(([A-Z]{2,5})\\)',             # (AAPL)\n",
        "            r'\\b([A-Z]{2,5})\\s+stock\\b',     # AAPL stock\n",
        "            r'\\b([A-Z]{2,5})\\s+shares?\\b',   # AAPL shares\n",
        "            r'\\b([A-Z]{2,5})\\s+ticker\\b',    # AAPL ticker\n",
        "        ]\n",
        "\n",
        "        for pattern in ticker_patterns:\n",
        "            matches = re.findall(pattern, query)\n",
        "            for match in matches:\n",
        "                if match in self.VALID_TICKERS and match not in self.EXCLUDED_WORDS:\n",
        "                    found_tickers.add(match)\n",
        "\n",
        "        # # 2. Fallback: use Finnhub symbol lookup\n",
        "        # if not found_tickers:\n",
        "        #     try:\n",
        "        #         simple_query_lower = query_lower.replace(\"'s\", \"\")\n",
        "        #         finnhub_client = finnhub.Client(api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
        "        #         for i in simple_query_lower.split(\" \"):\n",
        "        #             temp = finnhub_client.symbol_lookup(i)\n",
        "        #             if temp[\"count\"]>0 and i.upper() not in EXCLUDED_WORDS:\n",
        "        #                 found_tickers.add(temp[\"result\"][0][\"symbol\"])\n",
        "        #     except Exception:\n",
        "        #         pass  # Fail silently\n",
        "\n",
        "\n",
        "        # 3. Determine comparison mode (more accurate)\n",
        "        comparison_mode = False\n",
        "\n",
        "        # Check for explicit comparison language\n",
        "        for indicator in self.COMPARISON_INDICATORS:\n",
        "            if re.search(indicator, query_lower):\n",
        "                comparison_mode = True\n",
        "                break\n",
        "        ## Industry-specific fallbacks based on query context, default to MANGA\n",
        "        if len(found_tickers) == 0:\n",
        "            query_lower = query.lower()\n",
        "            if any(word in query_lower for word in ['technology', 'tech']):\n",
        "                found_tickers.update([\"AAPL\", \"MSFT\", \"GOOGL\", \"META\", \"NVDA\"])\n",
        "            elif any(word in query_lower for word in ['financial', 'bank']):\n",
        "                found_tickers.update([\"JPM\", \"BAC\", \"WFC\", \"GS\"])\n",
        "            elif any(word in query_lower for word in ['across industries', 'all companies', 'major companies']):\n",
        "                # Get all available companies from your database\n",
        "                found_tickers.update([\"AAPL\", \"MSFT\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\", \"AMZN\", \"NFLX\",\n",
        "                \"JPM\", \"BAC\", \"WFC\", \"GS\", \"JNJ\", \"PG\", \"KO\", \"BRK-A\"])\n",
        "            else:\n",
        "                # Default to your existing MANGA set\n",
        "                found_tickers.update([\"META\", \"AMZN\", \"NFLX\", \"GOOG\", \"AAPL\"])\n",
        "\n",
        "        # Check for multiple tickers\n",
        "        if len(found_tickers) > 1:\n",
        "            comparison_mode = True\n",
        "\n",
        "        # Check for conjunction patterns with company names\n",
        "        conjunction_pattern = r'\\b(?:apple|microsoft|google|amazon|tesla|meta)\\s+and\\s+(?:apple|microsoft|google|amazon|tesla|meta)\\b'\n",
        "        if re.search(conjunction_pattern, query_lower):\n",
        "            comparison_mode = True\n",
        "\n",
        "        return TickerContext(\n",
        "            tickers=frozenset(found_tickers),\n",
        "            comparison_mode=comparison_mode\n",
        "        )\n",
        "\n",
        "class TemporalExtractor:\n",
        "    \"\"\"Fixed temporal extraction with better quarter detection\"\"\"\n",
        "\n",
        "    def extract(self, query: str) -> TemporalContext:\n",
        "        \"\"\"Extract temporal context with improved parsing\"\"\"\n",
        "        query_lower = query.lower()\n",
        "        current_year = datetime.now().year\n",
        "\n",
        "        # 1. Specific quarter patterns (fix the major bug!)\n",
        "        quarter_patterns = [\n",
        "            r'\\bq([1-4])\\s+(20\\d{2})\\b',           # Q1 2023\n",
        "            r'\\b([1-4])q\\s+(20\\d{2})\\b',           # 1Q 2023\n",
        "            r'\\b(first|second|third|fourth)\\s+quarter\\s+(20\\d{2})\\b',  # first quarter 2023\n",
        "            r'\\b(q[1-4])\\s+(20\\d{2})\\b',           # q1 2023\n",
        "        ]\n",
        "\n",
        "        for pattern in quarter_patterns:\n",
        "            match = re.search(pattern, query_lower)\n",
        "            if match:\n",
        "                quarter_str = match.group(1)\n",
        "                year = int(match.group(2))\n",
        "\n",
        "                # Convert quarter to number\n",
        "                if quarter_str.isdigit():\n",
        "                    quarter = int(quarter_str)\n",
        "                elif quarter_str.startswith('q'):\n",
        "                    quarter = int(quarter_str[1])\n",
        "                else:\n",
        "                    quarter_map = {'first': 1, 'second': 2, 'third': 3, 'fourth': 4}\n",
        "                    quarter = quarter_map.get(quarter_str, 1)\n",
        "\n",
        "                return TemporalContext(\n",
        "                    scope=TemporalScope.SPECIFIC_QUARTER,\n",
        "                    specific_quarter=quarter,\n",
        "                    specific_year=year\n",
        "                )\n",
        "\n",
        "        # 2. Specific year patterns\n",
        "        year_patterns = [\n",
        "            r'\\b(20\\d{2})\\s+(?:annual|year|performance|results)\\b',  # 2022 annual\n",
        "            r'\\b(?:annual|year|performance|results)\\s+(20\\d{2})\\b',  # annual 2022\n",
        "            r'\\b(20\\d{2})\\s+(?:10-?k|annual\\s+report)\\b',          # 2022 10-K\n",
        "        ]\n",
        "\n",
        "        for pattern in year_patterns:\n",
        "            match = re.search(pattern, query_lower)\n",
        "            if match:\n",
        "                year = int(match.group(1))\n",
        "                return TemporalContext(\n",
        "                    scope=TemporalScope.SPECIFIC_YEAR,\n",
        "                    specific_year=year\n",
        "                )\n",
        "\n",
        "        # 3. Historical trend indicators\n",
        "        trend_indicators = [\n",
        "            r'\\bover\\s+time\\b', r'\\btrend\\b', r'\\bhistorical\\b',\n",
        "            r'\\bchanged?\\b', r'\\bevolution\\b', r'\\bhow\\s+has\\b',\n",
        "            r'\\btracked\\b', r'\\bprogression\\b', r'\\bdeveloped\\b'\n",
        "        ]\n",
        "\n",
        "        if any(re.search(indicator, query_lower) for indicator in trend_indicators):\n",
        "            return TemporalContext(scope=TemporalScope.HISTORICAL_TREND)\n",
        "\n",
        "        # 4. Recent indicators\n",
        "        recent_indicators = [\n",
        "            r'\\brecent\\b', r'\\blatest\\b', r'\\bcurrent\\b', r'\\bnew\\b',\n",
        "            r'\\blast\\b', r'\\bmost\\s+recent\\b', r'\\bup\\s+to\\s+date\\b'\n",
        "        ]\n",
        "\n",
        "        if any(re.search(indicator, query_lower) for indicator in recent_indicators):\n",
        "            return TemporalContext(scope=TemporalScope.RECENT)\n",
        "\n",
        "        # 5. Annual indicators\n",
        "        annual_indicators = [\n",
        "            r'\\bannual\\b', r'\\byearly\\b', r'\\bper\\s+year\\b', r'\\bannually\\b'\n",
        "        ]\n",
        "\n",
        "        if any(re.search(indicator, query_lower) for indicator in annual_indicators):\n",
        "            return TemporalContext(scope=TemporalScope.ANNUAL)\n",
        "\n",
        "        # 6. Quarterly indicators (general)\n",
        "        quarterly_indicators = [\n",
        "            r'\\bquarterly\\b', r'\\bquarter\\b', r'\\bq[1-4]\\b'\n",
        "        ]\n",
        "\n",
        "        if any(re.search(indicator, query_lower) for indicator in quarterly_indicators):\n",
        "            return TemporalContext(scope=TemporalScope.QUARTERLY)\n",
        "\n",
        "        # Default to recent\n",
        "        return TemporalContext(scope=TemporalScope.RECENT)\n",
        "\n",
        "class DocumentExtractor:\n",
        "    \"\"\"Fixed document extraction with better pattern matching\"\"\"\n",
        "\n",
        "    DOCUMENT_PATTERNS = {\n",
        "        DocumentScope.FORM_10K: [\n",
        "            r'\\b10-?k\\b', r'\\bannual\\s+report\\b', r'\\bannual\\s+filing\\b'\n",
        "        ],\n",
        "        DocumentScope.FORM_10Q: [\n",
        "            r'\\b10-?q\\b', r'\\bquarterly\\s+report\\b', r'\\bquarterly\\s+filing\\b'\n",
        "        ],\n",
        "        DocumentScope.FORM_8K: [\n",
        "            r'\\b8-?k\\b', r'\\bcurrent\\s+report\\b', r'\\bmaterial\\s+event\\b'\n",
        "        ],\n",
        "        DocumentScope.FORM_DEF14A: [\n",
        "            r'\\bdef\\s*14a\\b', r'\\bproxy\\s+statement\\b', r'\\bproxy\\b'\n",
        "        ],\n",
        "        DocumentScope.FORM_3: [\n",
        "            r'\\bform\\s+[3]\\b', r'\\binsider\\s+trading\\b', r'\\binsider\\s+transaction\\b',\n",
        "            r'\\bownership\\s+change\\b', r'\\bbeneficial\\s+ownership\\b'\n",
        "        ],\n",
        "        DocumentScope.FORM_4: [\n",
        "            r'\\bform\\s+[4]\\b', r'\\binsider\\s+trading\\b', r'\\binsider\\s+transaction\\b',\n",
        "            r'\\bownership\\s+change\\b', r'\\bbeneficial\\s+ownership\\b'\n",
        "        ],\n",
        "        DocumentScope.FORM_5: [\n",
        "            r'\\bform\\s+[5]\\b', r'\\binsider\\s+trading\\b', r'\\binsider\\s+transaction\\b',\n",
        "            r'\\bownership\\s+change\\b', r'\\bbeneficial\\s+ownership\\b'\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    def extract(self, query: str) -> DocumentContext:\n",
        "        \"\"\"Extract document context with improved patterns\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Check each document type\n",
        "        for doc_scope, patterns in self.DOCUMENT_PATTERNS.items():\n",
        "            if any(re.search(pattern, query_lower) for pattern in patterns):\n",
        "                return DocumentContext(scope=doc_scope)\n",
        "\n",
        "        return DocumentContext(scope=DocumentScope.ANY)\n",
        "\n",
        "\n",
        "class MultiDimensionalRouter:\n",
        "    \"\"\"Router with full ticker, temporal, and document awareness\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_router = CrossFormRouter()  # Your existing router\n",
        "        self.ticker_extractor = TickerExtractor()\n",
        "        self.temporal_extractor = TemporalExtractor()\n",
        "        self.document_extractor = DocumentExtractor()\n",
        "\n",
        "    def route_query(self, query: str, available_forms: Dict[str, Dict[str, List['FormChunk']]] = None) -> 'EnhancedRoutingResult':\n",
        "        \"\"\"\n",
        "        Route query with full multi-dimensional awareness\n",
        "\n",
        "        Args:\n",
        "            query: User's natural language query\n",
        "            available_forms: Dict[ticker][form_type] -> List[chunks]\n",
        "                           e.g., {\"AAPL\": {\"10-K\": [chunks], \"10-Q\": [chunks]}}\n",
        "\n",
        "        Returns:\n",
        "            Enhanced routing result with context-aware recommendations\n",
        "        \"\"\"\n",
        "        # Extract all context dimensions\n",
        "        ticker_context = self.ticker_extractor.extract(query)\n",
        "        temporal_context = self.temporal_extractor.extract(query)\n",
        "        document_context = self.document_extractor.extract(query)\n",
        "\n",
        "        query_context = QueryContext(\n",
        "            ticker_context=ticker_context,\n",
        "            temporal_context=temporal_context,\n",
        "            document_context=document_context,\n",
        "            original_query=query\n",
        "        )\n",
        "\n",
        "        # Get base routing from concept-based router\n",
        "        base_result = self.base_router.route_question(query)\n",
        "\n",
        "        # Apply multi-dimensional filtering and prioritization\n",
        "        enhanced_targets = self._apply_contextual_filtering(\n",
        "            base_result.primary_targets, query_context, available_forms\n",
        "        )\n",
        "\n",
        "        # Generate comprehensive reasoning\n",
        "        reasoning = self._generate_contextual_reasoning(query_context, enhanced_targets)\n",
        "\n",
        "        return EnhancedRoutingResult(\n",
        "            query_context=query_context,\n",
        "            primary_targets=enhanced_targets,\n",
        "            base_confidence_scores=base_result.confidence_scores,\n",
        "            reasoning=reasoning,\n",
        "            recommended_forms=self._get_recommended_forms(query_context),\n",
        "            recommended_tickers=list(ticker_context.tickers),\n",
        "            temporal_strategy=self._get_temporal_strategy(temporal_context)\n",
        "        )\n",
        "\n",
        "    def _apply_contextual_filtering(self, base_targets: List[SectionIdentifier],\n",
        "                                  context: QueryContext,\n",
        "                                  available_forms: Optional[Dict] = None) -> List[SectionIdentifier]:\n",
        "        \"\"\"Apply multi-dimensional filtering to base targets\"\"\"\n",
        "\n",
        "        filtered_targets = []\n",
        "\n",
        "        for target in base_targets:\n",
        "            # Document scope filtering\n",
        "            if self._matches_document_scope(target.form_type, context.document_context):\n",
        "                # Temporal preference adjustment\n",
        "                temporal_score = self._get_temporal_score(target.form_type, context.temporal_context)\n",
        "\n",
        "                if temporal_score > 0.3:  # Threshold for inclusion\n",
        "                    filtered_targets.append(target)\n",
        "\n",
        "        # Sort by contextual relevance\n",
        "        filtered_targets.sort(\n",
        "            key=lambda t: self._get_contextual_score(t, context),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Add fallback for unmatched queries\n",
        "        if not base_targets or not filtered_targets:\n",
        "            query_lower = context.original_query.lower()\n",
        "\n",
        "            # Check for common concepts that might be missed\n",
        "            concept_fallbacks = {\n",
        "                (\"r&d\", \"research\", \"development\", \"spending\"): \"research_development\",\n",
        "                (\"risk\", \"factor\", \"threat\", \"challenge\"): \"risk_factors\",\n",
        "                (\"revenue\", \"sales\", \"income\", \"changed\", \"over_time\"): \"revenue_analysis\"\n",
        "            }\n",
        "\n",
        "            for keywords, concept_name in concept_fallbacks.items():\n",
        "                if any(keyword in query_lower for keyword in keywords):\n",
        "                    # Find the matching ConceptMapping and add its sections\n",
        "                    matching_concept = next(\n",
        "                        (cm for cm in self.concept_mappings if cm.name == concept_name),\n",
        "                        None\n",
        "                    )\n",
        "                    if matching_concept:\n",
        "                        base_targets.extend(list(matching_concept.sections))\n",
        "                        break\n",
        "            return base_targets\n",
        "\n",
        "        return filtered_targets\n",
        "\n",
        "    def _matches_document_scope(self, form_type: FormType, doc_context: DocumentContext) -> bool:\n",
        "        \"\"\"Check if form type matches document scope\"\"\"\n",
        "        if doc_context.scope == DocumentScope.ANY:\n",
        "            return True\n",
        "\n",
        "        scope_mapping = {\n",
        "            DocumentScope.FORM_10K: FormType.FORM_10K,\n",
        "            DocumentScope.FORM_10Q: FormType.FORM_10Q,\n",
        "            DocumentScope.FORM_8K: FormType.FORM_8K,\n",
        "            DocumentScope.FORM_DEF14A: FormType.FORM_DEF14A,\n",
        "        }\n",
        "\n",
        "        if doc_context.scope in scope_mapping:\n",
        "            return form_type == scope_mapping[doc_context.scope]\n",
        "\n",
        "        if doc_context.scope == DocumentScope.FORMS_345:\n",
        "            return form_type in [FormType.FORM_3, FormType.FORM_4, FormType.FORM_5]\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _get_temporal_score(self, form_type: FormType, temporal_context: TemporalContext) -> float:\n",
        "        \"\"\"Calculate temporal alignment score\"\"\"\n",
        "        scope = temporal_context.scope\n",
        "\n",
        "        # Specific year/quarter preferences\n",
        "        if scope in [TemporalScope.SPECIFIC_YEAR, TemporalScope.ANNUAL]:\n",
        "            if form_type in [FormType.FORM_10K, FormType.FORM_DEF14A]:\n",
        "                return 1.0\n",
        "            elif form_type == FormType.FORM_10Q:\n",
        "                return 0.7  # Quarterly can supplement annual\n",
        "            else:\n",
        "                return 0.5\n",
        "\n",
        "        elif scope in [TemporalScope.SPECIFIC_QUARTER, TemporalScope.QUARTERLY]:\n",
        "            if form_type == FormType.FORM_10Q:\n",
        "                return 1.0\n",
        "            elif form_type == FormType.FORM_8K:\n",
        "                return 0.8  # 8-K can contain quarterly info\n",
        "            else:\n",
        "                return 0.4\n",
        "\n",
        "        elif scope == TemporalScope.RECENT:\n",
        "            if form_type in [FormType.FORM_8K, FormType.FORM_4]:\n",
        "                return 1.0\n",
        "            elif form_type == FormType.FORM_10Q:\n",
        "                return 0.8\n",
        "            else:\n",
        "                return 0.6\n",
        "\n",
        "        elif scope == TemporalScope.HISTORICAL_TREND:\n",
        "            # For trends, we want multiple form types\n",
        "            return 0.9  # All forms valuable for trends\n",
        "\n",
        "        return 0.7  # Default score\n",
        "\n",
        "    def _get_contextual_score(self, target: SectionIdentifier, context: QueryContext) -> float:\n",
        "        \"\"\"Calculate overall contextual relevance score\"\"\"\n",
        "        base_score = 1.0\n",
        "\n",
        "        # Temporal alignment\n",
        "        temporal_score = self._get_temporal_score(target.form_type, context.temporal_context)\n",
        "\n",
        "        # Document specificity bonus\n",
        "        doc_specificity = 1.2 if context.document_context.scope != DocumentScope.ANY else 1.0\n",
        "\n",
        "        # Comparison mode adjustments\n",
        "        comparison_bonus = 1.1 if context.ticker_context.comparison_mode else 1.0\n",
        "\n",
        "        return base_score * temporal_score * doc_specificity * comparison_bonus\n",
        "\n",
        "    def _get_recommended_forms(self, context: QueryContext) -> List[FormType]:\n",
        "        \"\"\"Get recommended form types based on context\"\"\"\n",
        "        if context.document_context.scope != DocumentScope.ANY:\n",
        "            # Specific document requested\n",
        "            scope_mapping = {\n",
        "                DocumentScope.FORM_10K: [FormType.FORM_10K],\n",
        "                DocumentScope.FORM_10Q: [FormType.FORM_10Q],\n",
        "                DocumentScope.FORM_8K: [FormType.FORM_8K],\n",
        "                DocumentScope.FORM_DEF14A: [FormType.FORM_DEF14A],\n",
        "                DocumentScope.FORMS_345: [FormType.FORM_3, FormType.FORM_4, FormType.FORM_5],\n",
        "            }\n",
        "            return scope_mapping.get(context.document_context.scope, [])\n",
        "\n",
        "        # Temporal-based recommendations\n",
        "        scope = context.temporal_context.scope\n",
        "\n",
        "        if scope == TemporalScope.HISTORICAL_TREND:\n",
        "            return [FormType.FORM_10K, FormType.FORM_10Q, FormType.FORM_8K]  # Multiple for trends\n",
        "        elif scope in [TemporalScope.SPECIFIC_YEAR, TemporalScope.ANNUAL]:\n",
        "            return [FormType.FORM_10K, FormType.FORM_DEF14A]\n",
        "        elif scope in [TemporalScope.SPECIFIC_QUARTER, TemporalScope.QUARTERLY]:\n",
        "            return [FormType.FORM_10Q]\n",
        "        elif scope == TemporalScope.RECENT:\n",
        "            return [FormType.FORM_8K, FormType.FORM_4, FormType.FORM_10Q]\n",
        "\n",
        "        return [FormType.FORM_10K, FormType.FORM_10Q, FormType.FORM_8K]  # Default\n",
        "\n",
        "    def _get_temporal_strategy(self, temporal_context: TemporalContext) -> str:\n",
        "        \"\"\"Get human-readable temporal strategy\"\"\"\n",
        "        scope = temporal_context.scope\n",
        "\n",
        "        if scope == TemporalScope.SPECIFIC_YEAR:\n",
        "            return f\"Focus on {temporal_context.specific_year} annual filings\"\n",
        "        elif scope == TemporalScope.SPECIFIC_QUARTER:\n",
        "            return f\"Focus on Q{temporal_context.specific_quarter} {temporal_context.specific_year} quarterly filing\"\n",
        "        elif scope == TemporalScope.HISTORICAL_TREND:\n",
        "            return \"Analyze across multiple periods for trend analysis\"\n",
        "        elif scope == TemporalScope.RECENT:\n",
        "            return \"Prioritize most recent filings\"\n",
        "        elif scope == TemporalScope.ANNUAL:\n",
        "            return \"Focus on annual reports (10-K, DEF 14A)\"\n",
        "        elif scope == TemporalScope.QUARTERLY:\n",
        "            return \"Focus on quarterly reports (10-Q)\"\n",
        "\n",
        "        return \"Standard temporal prioritization\"\n",
        "\n",
        "    def _generate_contextual_reasoning(self, context: QueryContext,\n",
        "                                     targets: List[SectionIdentifier]) -> str:\n",
        "        \"\"\"Generate comprehensive reasoning including context\"\"\"\n",
        "        reasoning_parts = []\n",
        "\n",
        "        # Ticker context\n",
        "        if context.ticker_context.tickers:\n",
        "            tickers = \", \".join(sorted(context.ticker_context.tickers))\n",
        "            if context.ticker_context.comparison_mode:\n",
        "                reasoning_parts.append(f\"Comparison analysis requested for: {tickers}\")\n",
        "            else:\n",
        "                reasoning_parts.append(f\"Analysis focused on: {tickers}\")\n",
        "\n",
        "        # Temporal context\n",
        "        temporal_strategy = self._get_temporal_strategy(context.temporal_context)\n",
        "        reasoning_parts.append(f\"Temporal strategy: {temporal_strategy}\")\n",
        "\n",
        "        # Document context\n",
        "        if context.document_context.scope != DocumentScope.ANY:\n",
        "            reasoning_parts.append(f\"Document scope: {context.document_context.scope.value}\")\n",
        "\n",
        "        # Target sections\n",
        "        if targets:\n",
        "            target_strs = [str(t) for t in targets[:3]]\n",
        "            reasoning_parts.append(f\"Primary sections: {', '.join(target_strs)}\")\n",
        "\n",
        "        return \". \".join(reasoning_parts)\n",
        "\n",
        "@dataclass\n",
        "class EnhancedRoutingResult:\n",
        "    \"\"\"Enhanced result with multi-dimensional context\"\"\"\n",
        "    query_context: QueryContext\n",
        "    primary_targets: List[SectionIdentifier]\n",
        "    base_confidence_scores: Dict[SectionIdentifier, float]\n",
        "    reasoning: str\n",
        "    recommended_forms: List[FormType]\n",
        "    recommended_tickers: List[str]\n",
        "    temporal_strategy: str\n",
        "\n"
      ],
      "metadata": {
        "id": "rULTY6mOm38r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Storage"
      ],
      "metadata": {
        "id": "LV8hDCa1E78P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import asyncio\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import hashlib\n",
        "import zlib\n",
        "from dataclasses import dataclass, field\n",
        "from contextlib import asynccontextmanager\n",
        "import threading\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import asyncpg\n",
        "import httpx\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict, List, Set, Optional, Protocol, Tuple, Union, Any\n",
        "from enum import Enum\n",
        "import calendar\n",
        "import re\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import jsonpickle\n",
        "from dateutil.parser import parse as dtparse\n",
        "from datetime import date, datetime, timedelta\n",
        "from functools import lru_cache\n",
        "\n",
        "# from chunking import *\n",
        "# from routing import *\n",
        "# from utilities import *\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# 1. TTL Cache (Fixes Memory Leak Issue)\n",
        "###############################################################################\n",
        "\n",
        "class TTLCache:\n",
        "    \"\"\"Thread-safe TTL cache that prevents memory leaks\"\"\"\n",
        "\n",
        "    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):\n",
        "        self.max_size = max_size\n",
        "        self.ttl_seconds = ttl_seconds\n",
        "        self._cache: Dict[str, Tuple[Any, float]] = {}\n",
        "        self._lock = threading.RLock()\n",
        "\n",
        "    def get(self, key: str) -> Optional[Any]:\n",
        "        with self._lock:\n",
        "            if key in self._cache:\n",
        "                value, expiry = self._cache[key]\n",
        "                if time.time() < expiry:\n",
        "                    return value\n",
        "                else:\n",
        "                    del self._cache[key]\n",
        "            return None\n",
        "\n",
        "    def put(self, key: str, value: Any) -> None:\n",
        "        with self._lock:\n",
        "            # Clean expired entries\n",
        "            current_time = time.time()\n",
        "            expired_keys = [k for k, (_, expiry) in self._cache.items() if current_time >= expiry]\n",
        "            for k in expired_keys:\n",
        "                del self._cache[k]\n",
        "\n",
        "            # Enforce size limit\n",
        "            if len(self._cache) >= self.max_size:\n",
        "                oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][1])\n",
        "                del self._cache[oldest_key]\n",
        "\n",
        "            self._cache[key] = (value, current_time + self.ttl_seconds)\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        with self._lock:\n",
        "            self._cache.clear()\n",
        "\n",
        "# Global caches (replaces problematic @lru_cache)\n",
        "ticker_cache = TTLCache(max_size=500, ttl_seconds=86400)  # 24h TTL\n",
        "embedding_cache = TTLCache(max_size=100, ttl_seconds=3600)  # 1h TTL\n",
        "query_cache = TTLCache(max_size=200, ttl_seconds=1800)  # 30min TTL\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3. Production Database (Fixes Connection Pool Issues)\n",
        "###############################################################################\n",
        "\n",
        "class ProductionDB:\n",
        "    \"\"\"Production database with proper connection management\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pool = None\n",
        "        self.sqlite_conn = None\n",
        "        self.provider = None\n",
        "\n",
        "    async def initialize(self) -> bool:\n",
        "        \"\"\"Initialize with robust fallback chain\"\"\"\n",
        "\n",
        "        # Try CockroachDB first (best free tier)\n",
        "        cockroach_url = os.getenv('COCKROACH_DATABASE_URL')\n",
        "        try:\n",
        "            self.pool = await asyncpg.create_pool(\n",
        "                cockroach_url,\n",
        "                min_size=5,\n",
        "                max_size=50,  # CockroachDB handles more connections\n",
        "                command_timeout=30,\n",
        "                max_inactive_connection_lifetime=300\n",
        "            )\n",
        "            await self._test_connection()\n",
        "            self.provider = \"cockroach\"\n",
        "            logger.info(\"✅ Connected to CockroachDB\")\n",
        "            await self._create_schema()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"CockroachDB failed: {e}\")\n",
        "\n",
        "\n",
        "    async def _test_connection(self):\n",
        "        \"\"\"Test database connection\"\"\"\n",
        "        async with self.pool.acquire() as conn:\n",
        "            await conn.fetchval(\"SELECT 1\")\n",
        "\n",
        "    async def _create_schema(self):\n",
        "        \"\"\"Create optimized schema for your routing system\"\"\"\n",
        "        schema = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS chunks (\n",
        "            chunk_id VARCHAR(255) PRIMARY KEY,\n",
        "            form_type VARCHAR(50) NOT NULL,\n",
        "            section_type VARCHAR(100) NOT NULL,\n",
        "            section_number VARCHAR(50) NOT NULL,\n",
        "            section_title TEXT NOT NULL,\n",
        "            content TEXT NOT NULL,\n",
        "            start_pos INTEGER NOT NULL,\n",
        "            end_pos INTEGER NOT NULL,\n",
        "\n",
        "            -- Your metadata structure preserved exactly\n",
        "            cik VARCHAR(20) NOT NULL,\n",
        "            ticker VARCHAR(10) NOT NULL,\n",
        "            filing_date DATE,\n",
        "            fiscal_year INTEGER,\n",
        "            fiscal_quarter INTEGER,\n",
        "            content_type VARCHAR(50),\n",
        "            char_count INTEGER NOT NULL,\n",
        "\n",
        "            filing_url TEXT,\n",
        "            document_url TEXT,\n",
        "\n",
        "            -- Attachment fields from your system\n",
        "            parent_form_type VARCHAR(50),\n",
        "            parent_filing_date DATE,\n",
        "            attachment_number VARCHAR(50),\n",
        "            attachment_description TEXT,\n",
        "            is_attachment BOOLEAN DEFAULT FALSE,\n",
        "            attachment_type VARCHAR(50),\n",
        "\n",
        "            -- Vector storage (quantized)\n",
        "            embedding BYTEA,\n",
        "            created_at TIMESTAMP DEFAULT NOW()\n",
        "        );\n",
        "\n",
        "        -- Indexes optimized for your routing queries\n",
        "        CREATE INDEX IF NOT EXISTS idx_routing_main ON chunks(ticker, form_type, fiscal_year DESC);\n",
        "        CREATE INDEX IF NOT EXISTS idx_sections ON chunks(form_type, section_type, section_number);\n",
        "        CREATE INDEX IF NOT EXISTS idx_temporal ON chunks(fiscal_year DESC, fiscal_quarter DESC);\n",
        "        CREATE INDEX IF NOT EXISTS idx_content_type ON chunks(content_type);\n",
        "        \"\"\"\n",
        "        async with self.pool.acquire() as conn:\n",
        "            await conn.execute(schema)\n",
        "\n",
        "    @asynccontextmanager\n",
        "    async def get_connection(self):\n",
        "        \"\"\"Connection context manager with proper error handling\"\"\"\n",
        "        try:\n",
        "            async with self.pool.acquire() as conn:\n",
        "                yield conn\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Database connection failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Add this helper function to your production code\n",
        "    def convert_date_fields(self, chunk: FormChunk) -> tuple:\n",
        "        \"\"\"Convert string dates to proper date objects for asyncpg\"\"\"\n",
        "        from datetime import datetime\n",
        "\n",
        "        def parse_date_safely(date_str):\n",
        "            if not date_str:\n",
        "                return None\n",
        "            if isinstance(date_str, str):\n",
        "                try:\n",
        "                    # Handle various date formats from SEC filings\n",
        "                    if 'T' in date_str:\n",
        "                        return datetime.fromisoformat(date_str.replace('Z', '+00:00')).date()\n",
        "                    else:\n",
        "                        # Try multiple date formats\n",
        "                        for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%Y-%m-%d %H:%M:%S']:\n",
        "                            try:\n",
        "                                return datetime.strptime(date_str, fmt).date()\n",
        "                            except ValueError:\n",
        "                                continue\n",
        "                        return None\n",
        "                except (ValueError, TypeError):\n",
        "                    return None\n",
        "            return date_str\n",
        "\n",
        "\n",
        "        # Convert the FormChunk date fields\n",
        "        filing_date = parse_date_safely(chunk.filing_date)\n",
        "        parent_filing_date = parse_date_safely(chunk.parent_filing_date)\n",
        "\n",
        "        return filing_date, parent_filing_date\n",
        "\n",
        "\n",
        "    async def store_chunks_batch(self, chunks: List[FormChunk], embeddings: List[bytes]) -> int:\n",
        "        \"\"\"Batch store using your FormChunk structure exactly\"\"\"\n",
        "        if not chunks or len(chunks) != len(embeddings):\n",
        "            return 0\n",
        "\n",
        "        success_count = 0\n",
        "\n",
        "        try:\n",
        "            async with self.get_connection() as conn:\n",
        "                # PostgreSQL batch insert\n",
        "                for chunk, embedding in zip(chunks, embeddings):\n",
        "                    # FIX: Convert dates properly\n",
        "                    chunk.filing_date, chunk.parent_filing_date = self.convert_date_fields(chunk)\n",
        "                    await conn.execute(\"\"\"\n",
        "                        INSERT INTO chunks (\n",
        "                            chunk_id, form_type, section_type, section_number, section_title,\n",
        "                            content, start_pos, end_pos, cik, ticker, filing_date, fiscal_year,\n",
        "                            fiscal_quarter, content_type, char_count, filing_url, document_url,\n",
        "                            parent_form_type, parent_filing_date, attachment_number,\n",
        "                            attachment_description, is_attachment, attachment_type, embedding\n",
        "                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23, $24)\n",
        "                        ON CONFLICT (chunk_id) DO UPDATE SET\n",
        "                            embedding = EXCLUDED.embedding,\n",
        "                            created_at = NOW()\n",
        "                    \"\"\",\n",
        "                    chunk.chunk_id, chunk.form_type, chunk.section_type,\n",
        "                    chunk.section_number, chunk.section_title, chunk.content,\n",
        "                    chunk.start_pos, chunk.end_pos, chunk.cik, chunk.ticker,\n",
        "                    chunk.filing_date, chunk.fiscal_year, chunk.fiscal_quarter,\n",
        "                    chunk.content_type, chunk.char_count, chunk.filing_url,\n",
        "                    chunk.document_url, chunk.parent_form_type, chunk.parent_filing_date,\n",
        "                    chunk.attachment_number, chunk.attachment_description,\n",
        "                    chunk.is_attachment, chunk.attachment_type, embedding\n",
        "                )\n",
        "                success_count = len(chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to store chunks: {e}\")\n",
        "            raise\n",
        "\n",
        "        return success_count\n",
        "\n",
        "    async def search_with_routing(self, routing_result: EnhancedRoutingResult, limit: int = 20) -> List[FormChunk]:\n",
        "        \"\"\"Search using YOUR routing intelligence - zero changes to your logic\"\"\"\n",
        "\n",
        "        conditions = [\"1=1\"]\n",
        "        params = []\n",
        "\n",
        "        # Your ticker filtering logic exactly\n",
        "        if routing_result.recommended_tickers:\n",
        "            if self.provider == \"sqlite\":\n",
        "                placeholders = ','.join('?' * len(routing_result.recommended_tickers))\n",
        "                conditions.append(f\"ticker IN ({placeholders})\")\n",
        "                params.extend(routing_result.recommended_tickers)\n",
        "            else:\n",
        "                conditions.append(\"ticker = ANY($\" + str(len(params) + 1) + \")\")\n",
        "                params.append(routing_result.recommended_tickers)\n",
        "\n",
        "        # Your form type filtering logic exactly\n",
        "        if routing_result.recommended_forms:\n",
        "            form_values = [f.value for f in routing_result.recommended_forms]\n",
        "            if self.provider == \"sqlite\":\n",
        "                placeholders = ','.join('?' * len(form_values))\n",
        "                conditions.append(f\"form_type IN ({placeholders})\")\n",
        "                params.extend(form_values)\n",
        "            else:\n",
        "                conditions.append(\"form_type = ANY($\" + str(len(params) + 1) + \")\")\n",
        "                params.append(form_values)\n",
        "\n",
        "        # Your temporal filtering logic exactly\n",
        "        temporal_ctx = routing_result.query_context.temporal_context\n",
        "        if temporal_ctx.specific_year:\n",
        "            if self.provider == \"sqlite\":\n",
        "                conditions.append(\"fiscal_year = ?\")\n",
        "                params.append(temporal_ctx.specific_year)\n",
        "            else:\n",
        "                conditions.append(\"fiscal_year = $\" + str(len(params) + 1))\n",
        "                params.append(temporal_ctx.specific_year)\n",
        "\n",
        "        # Your section targeting logic exactly\n",
        "        if routing_result.primary_targets:\n",
        "            section_conditions = []\n",
        "            for target in routing_result.primary_targets[:3]:\n",
        "                section_conditions.append(\n",
        "                    f\"(form_type = '{target.form_type.value}' AND section_type = '{target.section_type}')\"\n",
        "                )\n",
        "            if section_conditions:\n",
        "                conditions.append(f\"({' OR '.join(section_conditions)})\")\n",
        "\n",
        "        query = f\"\"\"\n",
        "            SELECT chunk_id, form_type, section_type, section_number, section_title,\n",
        "                   content, start_pos, end_pos, cik, ticker, filing_date, fiscal_year,\n",
        "                   fiscal_quarter, content_type, char_count, filing_url, document_url,\n",
        "                   parent_form_type, parent_filing_date, attachment_number,\n",
        "                   attachment_description, is_attachment, attachment_type, embedding\n",
        "            FROM chunks\n",
        "            WHERE {' AND '.join(conditions)}\n",
        "            ORDER BY fiscal_year DESC NULLS LAST, char_count DESC\n",
        "            LIMIT {limit}\n",
        "        \"\"\"\n",
        "\n",
        "        chunks = []\n",
        "        try:\n",
        "            async with self.get_connection() as conn:\n",
        "                if self.provider == \"sqlite\":\n",
        "                    async with conn.cursor() as cursor:\n",
        "                        await cursor.execute(query, params)\n",
        "                        rows = await cursor.fetchall()\n",
        "                        column_names = [desc[0] for desc in cursor.description]\n",
        "                        rows = [dict(zip(column_names, row)) for row in rows]\n",
        "                else:\n",
        "                    rows = await conn.fetch(query, *params)\n",
        "                    rows = [dict(row) for row in rows]\n",
        "\n",
        "                # Reconstruct your FormChunk objects exactly\n",
        "                for row in rows:\n",
        "                    chunk = FormChunk(\n",
        "                        form_type=row['form_type'],\n",
        "                        section_type=row['section_type'],\n",
        "                        section_number=row['section_number'],\n",
        "                        section_title=row['section_title'],\n",
        "                        content=row['content'],\n",
        "                        start_pos=row['start_pos'],\n",
        "                        end_pos=row['end_pos'],\n",
        "                        cik=row['cik'],\n",
        "                        ticker=row['ticker'],\n",
        "                        filing_date=row['filing_date'],\n",
        "                        fiscal_year=row['fiscal_year'],\n",
        "                        fiscal_quarter=row['fiscal_quarter'],\n",
        "                        chunk_id=row['chunk_id'],\n",
        "                        content_type=row['content_type'],\n",
        "                        char_count=row['char_count'],\n",
        "                        filing_url=row['filing_url'],\n",
        "                        document_url=row['document_url'],\n",
        "                        parent_form_type=row['parent_form_type'],\n",
        "                        parent_filing_date=row['parent_filing_date'],\n",
        "                        attachment_number=row['attachment_number'],\n",
        "                        attachment_description=row['attachment_description'],\n",
        "                        is_attachment=row['is_attachment'],\n",
        "                        attachment_type=row['attachment_type']\n",
        "                    )\n",
        "                    chunks.append(chunk)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Search failed: {e}\")\n",
        "            raise\n",
        "\n",
        "        return chunks\n",
        "\n",
        "###############################################################################\n",
        "# 4. Memory-Efficient Vector Store (Fixes Memory Issues)\n",
        "###############################################################################\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"Memory-efficient vector store with quantization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self._device = \"cpu\"  # Conservative default\n",
        "\n",
        "    def _get_model(self) -> SentenceTransformer:\n",
        "        \"\"\"Lazy load with memory management\"\"\"\n",
        "        if self.model is None:\n",
        "            self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "            # self.model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
        "            # self.model.eval()\n",
        "        return self.model\n",
        "\n",
        "    def _quantize_embedding(self, embedding: np.ndarray) -> bytes:\n",
        "        \"\"\"Quantize to int8 for 4x storage reduction\"\"\"\n",
        "        normalized = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
        "        quantized = (normalized * 127).astype(np.int8)\n",
        "        return zlib.compress(quantized.tobytes(), level=6)\n",
        "\n",
        "    def _dequantize_embedding(self, compressed_bytes: bytes) -> np.ndarray:\n",
        "        \"\"\"Restore quantized embedding\"\"\"\n",
        "        quantized_bytes = zlib.decompress(compressed_bytes)\n",
        "        quantized = np.frombuffer(quantized_bytes, dtype=np.int8)\n",
        "        return quantized.astype(np.float32) / 127.0\n",
        "\n",
        "    async def embed_texts_batch(self, texts: List[str]) -> List[bytes]:\n",
        "        \"\"\"Batch embedding with caching\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        for text in texts:\n",
        "            # Check cache first\n",
        "            cache_key = hashlib.md5(text.encode()).hexdigest()\n",
        "            cached = embedding_cache.get(cache_key)\n",
        "\n",
        "            if cached:\n",
        "                embeddings.append(cached)\n",
        "            else:\n",
        "                # Generate embedding\n",
        "                model = self._get_model()\n",
        "                text_truncated = text[:2048]  # Prevent memory issues\n",
        "\n",
        "                try:\n",
        "                    loop = asyncio.get_event_loop()\n",
        "                    # embedding = await loop.run_in_executor(\n",
        "                    #     None,\n",
        "                    #     lambda: model.embed_query(text_truncated)\n",
        "                    # )\n",
        "                    embedding = await loop.run_in_executor(\n",
        "                        None,\n",
        "                        lambda: model.encode([text_truncated], show_progress_bar=False)[0]\n",
        "                    )\n",
        "                    quantized = self._quantize_embedding(embedding)\n",
        "                    embedding_cache.put(cache_key, quantized)\n",
        "                    embeddings.append(quantized)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Embedding failed for text: {e}\")\n",
        "                    # Fallback: zero embedding\n",
        "                    zero_emb = np.zeros(384, dtype=np.float32)\n",
        "                    quantized = self._quantize_embedding(zero_emb)\n",
        "                    embeddings.append(quantized)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    async def similarity_search(\n",
        "        self,\n",
        "        query_text: str,\n",
        "        candidate_chunks: List[FormChunk],\n",
        "        top_k: int = 10\n",
        "    ) -> List[Tuple[FormChunk, float]]:\n",
        "        \"\"\"Semantic similarity ranking\"\"\"\n",
        "\n",
        "        if not candidate_chunks:\n",
        "            return []\n",
        "\n",
        "        # Get query embedding\n",
        "        query_embeddings = await self.embed_texts_batch([query_text])\n",
        "        query_vec = self._dequantize_embedding(query_embeddings[0])\n",
        "\n",
        "        # Score candidates\n",
        "        scored_chunks = []\n",
        "\n",
        "        for chunk in candidate_chunks:\n",
        "            # Simple TF-IDF-like scoring as fallback if no embeddings\n",
        "            query_words = set(query_text.lower().split())\n",
        "            content_words = set(chunk.content.lower().split())\n",
        "            overlap = len(query_words.intersection(content_words))\n",
        "\n",
        "            # Boost based on your content type system\n",
        "            type_boost = 1.0\n",
        "            if chunk.content_type in ['risk_factors', 'mda', 'business_overview']:\n",
        "                type_boost = 1.2\n",
        "\n",
        "            score = (overlap / max(len(query_words), 1)) * type_boost\n",
        "            scored_chunks.append((chunk, score))\n",
        "\n",
        "        # Return top-k\n",
        "        scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scored_chunks[:top_k]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 6. Production Ingestion Pipeline (Uses Your Logic Exactly)\n",
        "###############################################################################\n",
        "\n",
        "class IngestionPipeline:\n",
        "    \"\"\"Production ingestion using YOUR chunking logic exactly\"\"\"\n",
        "\n",
        "    def __init__(self, db: ProductionDB, vector_store: VectorStore):\n",
        "        self.db = db\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    async def ingest_company(\n",
        "        self,\n",
        "        ticker: str,\n",
        "        forms: List[str] = [\"10-K\", \"10-Q\", \"8-K\"],\n",
        "        limit: int = 5\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Ingest company using YOUR exact chunking and metadata logic\"\"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = {\n",
        "            \"ticker\": ticker,\n",
        "            \"chunks_processed\": 0,\n",
        "            \"filings_processed\": 0,\n",
        "            \"errors\": []\n",
        "        }\n",
        "        # sec_limiter = SECRateLimiter()\n",
        "        try:\n",
        "            company = Company(ticker)\n",
        "            filings = company.get_filings(form=forms).head(5)\n",
        "\n",
        "            logger.info(f\"Processing {len(filings)} filings for {ticker}\")\n",
        "\n",
        "            for filing in filings:\n",
        "                try:\n",
        "                    # await sec_limiter.acquire()  # Rate limit each request\n",
        "\n",
        "                    # Get filing text\n",
        "                    main_text = filing.text()\n",
        "\n",
        "                    if len(main_text.strip()) < 10:\n",
        "                        continue\n",
        "\n",
        "                    # Use YOUR metadata extraction exactly\n",
        "                    base_meta = build_base_metadata(filing, main_text)\n",
        "\n",
        "                    # Use YOUR chunking logic exactly\n",
        "                    main_meta = base_meta.copy()\n",
        "                    main_meta.pop('form_type', None)\n",
        "                    main_chunks = chunk_form(main_text, filing.form, **main_meta)\n",
        "\n",
        "                    # Use YOUR attachment processing exactly\n",
        "                    attachment_chunks = process_filing_attachments(filing, base_meta)\n",
        "\n",
        "                    all_chunks = main_chunks + attachment_chunks\n",
        "\n",
        "                    # Generate embeddings\n",
        "                    chunk_texts = [chunk.content[:2048] for chunk in all_chunks]\n",
        "                    embeddings = await self.vector_store.embed_texts_batch(chunk_texts)\n",
        "\n",
        "                    # Store in database\n",
        "                    stored_count = await self.db.store_chunks_batch(all_chunks, embeddings)\n",
        "\n",
        "                    result[\"chunks_processed\"] += stored_count\n",
        "                    result[\"filings_processed\"] += 1\n",
        "\n",
        "                    logger.info(f\"Processed {filing.form} for {ticker}: {stored_count} chunks\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    result[\"errors\"].append(f\"Failed to process {filing.form}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            result[\"errors\"].append(f\"Failed to get filings for {ticker}: {str(e)}\")\n",
        "\n",
        "        result[\"processing_time\"] = time.time() - start_time\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "EWc4Inz4m4AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline"
      ],
      "metadata": {
        "id": "ObQqK5_aFatV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import asyncio\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import hashlib\n",
        "import zlib\n",
        "from dataclasses import dataclass, field\n",
        "from contextlib import asynccontextmanager\n",
        "import threading\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import asyncpg\n",
        "import httpx\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict, List, Set, Optional, Protocol, Tuple, Union, Any\n",
        "from enum import Enum\n",
        "import calendar\n",
        "import re\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import jsonpickle\n",
        "from dateutil.parser import parse as dtparse\n",
        "from datetime import date, datetime, timedelta\n",
        "from functools import lru_cache\n",
        "\n",
        "# from chunking import *\n",
        "# from routing import *\n",
        "# from utilities import *\n",
        "# from storage import *\n",
        "\n",
        "# from groq import AsyncGroq\n",
        "from google import genai\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LLMResponse:\n",
        "    text: str\n",
        "    provider: str\n",
        "    success: bool = True\n",
        "\n",
        "###############################################################################\n",
        "# 7. Complete QA Engine (Uses Your Router Exactly)\n",
        "###############################################################################\n",
        "\n",
        "@dataclass\n",
        "class QAResult:\n",
        "    answer: str\n",
        "    sources: List[Dict[str, Any]]\n",
        "    confidence: float\n",
        "    routing_info: Dict[str, Any]\n",
        "    provider_used: str\n",
        "    processing_time: float\n",
        "\n",
        "\n",
        "class LLMProvider:\n",
        "    \"\"\"Rate-limited LLM provider with fallback\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # self.groq_client = AsyncGroq()\n",
        "        self.gemini_client = genai.Client()\n",
        "        self._last_request = {}\n",
        "        # self._min_interval = {\"groq\": 2.0}  # Conservative intervals\n",
        "\n",
        "    # async def _rate_limit(self, provider: str):\n",
        "    #     \"\"\"Simple rate limiting\"\"\"\n",
        "    #     if provider in self._last_request:\n",
        "    #         elapsed = time.time() - self._last_request[provider]\n",
        "    #         min_interval = self._min_interval.get(provider, 1.0)\n",
        "    #         if elapsed < min_interval:\n",
        "    #             await asyncio.sleep(min_interval - elapsed)\n",
        "\n",
        "    #     self._last_request[provider] = time.time()\n",
        "\n",
        "    async def generate(self, messages: List[Dict[str, str]]) -> LLMResponse:\n",
        "        \"\"\"Generate with fallback chain\"\"\"\n",
        "\n",
        "        # groq_key = os.getenv('GROQ_API_KEY')\n",
        "        gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not gemini_key:\n",
        "          return LLMResponse(\n",
        "              text=\"No API key configured for LLM provider.\",\n",
        "              provider=\"error\",\n",
        "              success=False\n",
        "          )\n",
        "        # if groq_key:\n",
        "        if gemini_key:\n",
        "            try:\n",
        "              # completion = await self.groq_client.chat.completions.create(\n",
        "              #     model=\"llama-3.3-70b-versatile\",\n",
        "              #     messages=messages,\n",
        "              #     temperature=0.1,\n",
        "              #     max_completion_tokens=1024\n",
        "              # )\n",
        "\n",
        "                #  Extract messages by role\n",
        "                system_msgs = [msg[\"content\"] for msg in messages if msg[\"role\"] == \"system\"]\n",
        "                user_msgs = [msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"]\n",
        "\n",
        "                # Combine content\n",
        "                system_part = \"\\n\".join(system_msgs) if system_msgs else \"\"\n",
        "                user_part = \"\\n\".join(user_msgs) if user_msgs else \"\"\n",
        "                final_content = f\"{system_part}\\n\\n{user_part}\" if system_part else user_part\n",
        "\n",
        "                response = self.gemini_client.models.generate_content(\n",
        "                    model=\"gemini-2.5-flash\",\n",
        "                    contents=final_content\n",
        "                )\n",
        "                return LLMResponse(\n",
        "                  # text=completion.choices[0].message.content,\n",
        "                  text = response.text,\n",
        "                  provider=\"gemini\",\n",
        "                )\n",
        "\n",
        "            except Exception as e:\n",
        "                    # logger.warning(f\"Groq failed: {e}\")\n",
        "                    logger.warning(f\"Gemini failed: {e}\")\n",
        "\n",
        "        # Extractive fallback\n",
        "        return LLMResponse(\n",
        "            text=\"I apologize, but I'm experiencing technical difficulties with LLM providers. Please try again later.\",\n",
        "            provider=\"fallback\",\n",
        "            success=False\n",
        "        )\n",
        "\n",
        "\n",
        "class SECQASystem:\n",
        "    \"\"\"Main application class - production ready\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.db = ProductionDB()\n",
        "        self.vector_store = VectorStore()\n",
        "        self.llm = LLMProvider()\n",
        "        self.ingestion = None\n",
        "        self.qa = None\n",
        "        self.initialized = False\n",
        "\n",
        "    async def initialize(self) -> bool:\n",
        "        \"\"\"Initialize all components\"\"\"\n",
        "        try:\n",
        "            # Initialize database\n",
        "            if not await self.db.initialize():\n",
        "                logger.error(\"Database initialization failed\")\n",
        "                return False\n",
        "\n",
        "            # Initialize other components\n",
        "            self.ingestion = IngestionPipeline(self.db, self.vector_store)\n",
        "            self.qa = QAEngine(self.db, self.vector_store, self.llm)\n",
        "\n",
        "            self.initialized = True\n",
        "            logger.info(\"✅ SEC QA System initialized successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Initialization failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _check_initialized(self):\n",
        "        \"\"\"Check if system is initialized\"\"\"\n",
        "        if not self.initialized:\n",
        "            raise RuntimeError(\"System not initialized. Call initialize() first.\")\n",
        "\n",
        "    async def ingest_companies(\n",
        "        self,\n",
        "        tickers: List[str],\n",
        "        forms: List[str] = [\"10-K\", \"10-Q\", \"8-K\"],\n",
        "        limit_per_company: int = 5\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Ingest multiple companies\"\"\"\n",
        "        self._check_initialized()\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for ticker in tickers:\n",
        "            try:\n",
        "                result = await self.ingestion.ingest_company(ticker, forms, limit_per_company)\n",
        "                results[ticker] = result\n",
        "\n",
        "                logger.info(f\"Ingested {ticker}: {result['chunks_processed']} chunks\")\n",
        "\n",
        "                # Brief pause between companies\n",
        "                await asyncio.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                results[ticker] = {\n",
        "                    \"ticker\": ticker,\n",
        "                    \"chunks_processed\": 0,\n",
        "                    \"filings_processed\": 0,\n",
        "                    \"errors\": [f\"Failed to process {ticker}: {str(e)}\"],\n",
        "                    \"processing_time\": 0\n",
        "                }\n",
        "\n",
        "        total_chunks = sum(r.get('chunks_processed', 0) for r in results.values())\n",
        "        logger.info(f\"✅ Batch ingestion complete: {total_chunks} total chunks\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def ask_question(self, question: str, max_sources: int = 8) -> QAResult:\n",
        "        \"\"\"Ask a question using your complete routing intelligence\"\"\"\n",
        "        self._check_initialized()\n",
        "        return await self.qa.ask(question, max_sources)\n",
        "\n",
        "    async def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system status\"\"\"\n",
        "        self._check_initialized()\n",
        "\n",
        "        try:\n",
        "            async with self.db.get_connection() as conn:\n",
        "                chunk_count = await conn.fetchval(\"SELECT COUNT(*) FROM chunks\")\n",
        "                ticker_count = await conn.fetchval(\"SELECT COUNT(DISTINCT ticker) FROM chunks\")\n",
        "\n",
        "            return {\n",
        "                \"status\": \"healthy\",\n",
        "                \"database_provider\": self.db.provider,\n",
        "                \"chunks_stored\": chunk_count,\n",
        "                \"companies_indexed\": ticker_count,\n",
        "                \"cache_stats\": {\n",
        "                    \"ticker_cache_size\": len(ticker_cache._cache),\n",
        "                    \"embedding_cache_size\": len(embedding_cache._cache),\n",
        "                    \"query_cache_size\": len(query_cache._cache)\n",
        "                },\n",
        "                \"features\": [\n",
        "                    \"Your FormChunk chunking with full metadata\",\n",
        "                    \"Your MultiDimensionalRouter with context awareness\",\n",
        "                    \"TTL caching to prevent memory leaks\",\n",
        "                    \"SEC rate limiting\",\n",
        "                    \"Quantized embeddings for efficiency\",\n",
        "                    \"Multi-provider LLM fallback\"\n",
        "                ]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "\n",
        "class QAEngine:\n",
        "    \"\"\"Complete QA engine built on YOUR routing intelligence\"\"\"\n",
        "\n",
        "    def __init__(self, db: ProductionDB, vector_store: VectorStore, llm: LLMProvider):\n",
        "        self.db = db\n",
        "        self.vector_store = vector_store\n",
        "        self.llm = llm\n",
        "        # Use YOUR router exactly - zero modifications\n",
        "        self.router = MultiDimensionalRouter()\n",
        "\n",
        "    # ADD this to the existing QAEngine class\n",
        "    async def smart_ingest_for_query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Use YOUR router to determine what data to fetch\"\"\"\n",
        "\n",
        "        # 1. Use your existing router\n",
        "        routing_result = self.router.route_query(question)\n",
        "\n",
        "        # 2. Check what temporal data your router identified as needed\n",
        "        temporal_ctx = routing_result.query_context.temporal_context\n",
        "        tickers = routing_result.recommended_tickers\n",
        "        forms = [f.value for f in routing_result.recommended_forms]\n",
        "\n",
        "        logger.info(f\"🎯 Router identified need for: {tickers}, {forms}, {temporal_ctx.scope.value}\")\n",
        "\n",
        "        # 3. Check existing data in your CockroachDB\n",
        "        async with self.db.pool.acquire() as conn:\n",
        "            existing_data = await conn.fetch(\"\"\"\n",
        "                SELECT ticker, fiscal_year, COUNT(*) as chunk_count\n",
        "                FROM chunks\n",
        "                WHERE ticker = ANY($1) AND form_type = ANY($2)\n",
        "                GROUP BY ticker, fiscal_year\n",
        "                ORDER BY fiscal_year DESC\n",
        "            \"\"\", tickers, forms)\n",
        "\n",
        "        # 4. Determine what years need more data based on router intelligence\n",
        "        needed_years = set()\n",
        "\n",
        "        if temporal_ctx.specific_year:\n",
        "            needed_years.add(temporal_ctx.specific_year)\n",
        "        elif temporal_ctx.scope.value == \"historical\":\n",
        "            # Router wants historical analysis - need multiple years\n",
        "            needed_years.update([2020, 2021, 2022, 2023, 2024])\n",
        "        else:\n",
        "            # Recent data\n",
        "            needed_years.update([2023, 2024])\n",
        "\n",
        "        # 5. Fetch targeted filings based on router needs\n",
        "        total_new_chunks = 0\n",
        "\n",
        "        for ticker in tickers:\n",
        "            try:\n",
        "                # await self._rate_limit_sec()\n",
        "                company = Company(ticker)\n",
        "\n",
        "                # Get filings for the years your router identified\n",
        "                for year in needed_years:\n",
        "                    try:\n",
        "                        # Fetch specific year data based on router intelligence\n",
        "                        year_filings = company.get_filings(\n",
        "                            form=forms,\n",
        "                            filing_date=f\"{year}-01-01:{year}-12-31\"\n",
        "                        )  # Conservative limit per year\n",
        "\n",
        "                        for filing in year_filings:\n",
        "                            try:\n",
        "                                # await self._rate_limit_sec()\n",
        "                                main_text = filing.text()\n",
        "\n",
        "                                if len(main_text.strip()) < 100:\n",
        "                                    continue\n",
        "\n",
        "                                # Use YOUR existing chunking logic exactly\n",
        "                                base_meta = build_base_metadata(filing, main_text)\n",
        "                                main_meta = base_meta.copy()\n",
        "                                main_meta.pop('form_type', None)\n",
        "                                main_chunks = chunk_form(main_text, filing.form, **main_meta)\n",
        "                                attachment_chunks = process_filing_attachments(filing, base_meta)\n",
        "\n",
        "                                all_chunks = main_chunks + attachment_chunks\n",
        "\n",
        "                                # Generate embeddings and store using your existing methods\n",
        "                                chunk_texts = [chunk.content for chunk in all_chunks]\n",
        "                                embeddings = self.vector_store.embed_texts(chunk_texts)\n",
        "\n",
        "                                # Use your existing storage method (now with date fix)\n",
        "                                stored_count = await self.db.store_chunks_batch(all_chunks, embeddings)\n",
        "                                total_new_chunks += stored_count\n",
        "\n",
        "                                logger.info(f\"📊 {ticker} {year}: +{stored_count} chunks\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                logger.error(f\"Error processing {ticker} {year} {filing.form}: {e}\")\n",
        "                                continue\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"No {year} data for {ticker}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error fetching {ticker}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return {\n",
        "            \"new_chunks\": total_new_chunks,\n",
        "            \"router_reasoning\": routing_result.reasoning,\n",
        "            \"temporal_strategy\": temporal_ctx.scope.value\n",
        "        }\n",
        "\n",
        "\n",
        "    async def ask(self, question: str, max_sources: int = 8) -> QAResult:\n",
        "        \"\"\"End-to-end QA using YOUR routing logic exactly\"\"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Check cache first\n",
        "        cache_key = hashlib.md5(question.encode()).hexdigest()\n",
        "        cached_result = query_cache.get(cache_key)\n",
        "        if cached_result:\n",
        "            logger.info(f\"Cache hit for question: {question[:50]}...\")\n",
        "            return cached_result\n",
        "\n",
        "        logger.info(f\"Processing question: {question}\")\n",
        "\n",
        "        # 1. Use YOUR routing logic exactly - zero changes\n",
        "        routing_result = self.router.route_query(question)\n",
        "\n",
        "        logger.info(f\"Routing: {routing_result.reasoning}\")\n",
        "\n",
        "        if not routing_result.primary_targets:\n",
        "            return QAResult(\n",
        "                answer=\"I couldn't identify specific sections relevant to your question. Please try rephrasing.\",\n",
        "                sources=[],\n",
        "                confidence=0.0,\n",
        "                routing_info={\"reasoning\": routing_result.reasoning},\n",
        "                provider_used=\"none\",\n",
        "                processing_time=time.time() - start_time\n",
        "            )\n",
        "\n",
        "        # 2. Search database using YOUR routing result exactly\n",
        "        try:\n",
        "            candidate_chunks = await self.db.search_with_routing(routing_result, limit=max_sources * 2)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Database search failed: {e}\")\n",
        "            return QAResult(\n",
        "                answer=\"I experienced a technical issue accessing the database. Please try again.\",\n",
        "                sources=[],\n",
        "                confidence=0.2,\n",
        "                routing_info={\"error\": str(e)},\n",
        "                provider_used=\"none\",\n",
        "                processing_time=time.time() - start_time\n",
        "            )\n",
        "\n",
        "        # 2.5: If insufficient data, use router intelligence to fetch more\n",
        "        if len(candidate_chunks) < max_sources // 2:  # Less than half needed\n",
        "            logger.info(\"🎯 Insufficient data - using router to fetch targeted data\")\n",
        "            ingest_result = await self.smart_ingest_for_query(question)\n",
        "            logger.info(f\"✅ Router-driven ingestion: +{ingest_result['new_chunks']} chunks\")\n",
        "\n",
        "            candidate_chunks = await self.db.search_with_routing(routing_result, limit=max_sources * 2)\n",
        "\n",
        "        if not candidate_chunks:\n",
        "            return QAResult(\n",
        "                answer=\"I couldn't find relevant information in the database for your question.\",\n",
        "                sources=[],\n",
        "                confidence=0.3,\n",
        "                routing_info={\"reasoning\": routing_result.reasoning},\n",
        "                provider_used=\"none\",\n",
        "                processing_time=time.time() - start_time\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Found {len(candidate_chunks)} candidate chunks\")\n",
        "\n",
        "        # 3. Semantic ranking\n",
        "        try:\n",
        "            ranked_chunks = await self.vector_store.similarity_search(\n",
        "                question,\n",
        "                candidate_chunks,\n",
        "                top_k=max_sources\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Semantic search failed: {e}\")\n",
        "            # Fallback to routing order\n",
        "            ranked_chunks = [(chunk, 0.5) for chunk in candidate_chunks[:max_sources]]\n",
        "\n",
        "        if not ranked_chunks:\n",
        "            return QAResult(\n",
        "                answer=\"I found some potentially relevant information but couldn't rank it properly.\",\n",
        "                sources=[],\n",
        "                confidence=0.3,\n",
        "                routing_info={\"reasoning\": routing_result.reasoning},\n",
        "                provider_used=\"none\",\n",
        "                processing_time=time.time() - start_time\n",
        "            )\n",
        "\n",
        "        # 4. Build context from YOUR FormChunk structure exactly\n",
        "        context_parts = []\n",
        "        sources = []\n",
        "\n",
        "        for i, (chunk, score) in enumerate(ranked_chunks, 1):\n",
        "            # Use YOUR complete metadata structure\n",
        "            context_header = f\"[{i}] {chunk.ticker} {chunk.form_type}\"\n",
        "\n",
        "            if chunk.fiscal_year:\n",
        "                context_header += f\" (FY{chunk.fiscal_year}\"\n",
        "                if chunk.fiscal_quarter:\n",
        "                    context_header += f\" Q{chunk.fiscal_quarter}\"\n",
        "                context_header += \")\"\n",
        "\n",
        "            context_header += f\" - {chunk.section_title}\"\n",
        "\n",
        "            # Add content with reasonable limit\n",
        "            # content_preview = chunk.content[:1000] + \"...\" if len(chunk.content) > 1000 else chunk.content\n",
        "            content_preview = chunk.content\n",
        "            context_parts.append(f\"{context_header}:\\n{content_preview}\")\n",
        "\n",
        "            # Build source info from YOUR metadata exactly\n",
        "            sources.append({\n",
        "                \"id\": str(i),\n",
        "                \"ticker\": chunk.ticker,\n",
        "                \"form_type\": chunk.form_type,\n",
        "                \"fiscal_year\": chunk.fiscal_year,\n",
        "                \"fiscal_quarter\": chunk.fiscal_quarter,\n",
        "                \"section\": f\"{chunk.section_type} {chunk.section_number}\".strip(),\n",
        "                \"section_title\": chunk.section_title,\n",
        "                \"filing_date\": chunk.filing_date,\n",
        "                \"content_type\": chunk.content_type,\n",
        "                \"document_url\": chunk.document_url or chunk.filing_url,\n",
        "                \"is_attachment\": chunk.is_attachment,\n",
        "                \"similarity_score\": score\n",
        "            })\n",
        "\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        # 5. Generate answer\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a financial research assistant analyzing SEC filings. \"\n",
        "                    \"Provide accurate, detailed answers based strictly on the provided context. \"\n",
        "                    \"Always cite sources using [1], [2], etc. numbers. \"\n",
        "                    \"Be specific about which filing and section information comes from. \"\n",
        "                    \"If information is incomplete, clearly state what's missing.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer with citations:\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            llm_response = await self.llm.generate(messages)\n",
        "\n",
        "            # Calculate confidence\n",
        "            confidence = 0.8 if llm_response.success else 0.4\n",
        "            if len(ranked_chunks) >= 3:\n",
        "                confidence *= 1.1\n",
        "            if routing_result.recommended_tickers:\n",
        "                confidence *= 1.1\n",
        "\n",
        "            confidence = min(confidence, 1.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"LLM generation failed: {e}\")\n",
        "            llm_response = LLMResponse(\n",
        "                text=\"I found relevant information but couldn't generate a proper response due to technical issues.\",\n",
        "                provider=\"error\",\n",
        "                success=False\n",
        "            )\n",
        "            confidence = 0.3\n",
        "\n",
        "        result = QAResult(\n",
        "            answer=llm_response.text,\n",
        "            sources=sources,\n",
        "            confidence=confidence,\n",
        "            routing_info={\n",
        "                \"reasoning\": routing_result.reasoning,\n",
        "                \"recommended_tickers\": routing_result.recommended_tickers,\n",
        "                \"recommended_forms\": [f.value for f in routing_result.recommended_forms],\n",
        "                \"temporal_strategy\": routing_result.temporal_strategy\n",
        "            },\n",
        "            provider_used=llm_response.provider,\n",
        "            processing_time=time.time() - start_time\n",
        "        )\n",
        "\n",
        "        # Cache result\n",
        "        query_cache.put(cache_key, result)\n",
        "\n",
        "        logger.info(f\"Generated answer using {llm_response.provider} in {result.processing_time:.1f}s\")\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "l_f0kRQ4nEcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### .."
      ],
      "metadata": {
        "id": "44nwwjYHFdJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####.."
      ],
      "metadata": {
        "id": "W3i7CHJEmst_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -V\n",
        "\n",
        "os.environ['EDGAR_IDENTITY'] = 'CompanyName CompanyEmail'\n",
        "os.environ['GEMINI_API_KEY'] = 'GEMINI_API_KEY'\n",
        "os.environ['COCKROACH_DATABASE_URL'] = 'COCKROACH_DATABASE_URL'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEd2l5tbas7Z",
        "outputId": "3508fe03-8ba8-4c40-877b-c88272973306"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import asyncio\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "import hashlib\n",
        "import zlib\n",
        "from dataclasses import dataclass, field\n",
        "from contextlib import asynccontextmanager\n",
        "import threading\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import asyncpg\n",
        "import httpx\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict, List, Set, Optional, Protocol, Tuple, Union, Any\n",
        "from enum import Enum\n",
        "import calendar\n",
        "import re\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import jsonpickle\n",
        "from dateutil.parser import parse as dtparse\n",
        "from datetime import date, datetime, timedelta\n",
        "from functools import lru_cache\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# from chunking import *\n",
        "# from routing import *\n",
        "# from utilities import *\n",
        "# from storage import *\n",
        "# from pipeline import *\n",
        "# from edgar import *\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "###############################################################################\n",
        "# 9. Demo Script\n",
        "###############################################################################\n",
        "\n",
        "async def demo_system():\n",
        "    \"\"\"Demo the production system\"\"\"\n",
        "\n",
        "    # Initialize system\n",
        "    system = SECQASystem()\n",
        "\n",
        "    if not await system.initialize():\n",
        "        print(\"❌ System initialization failed\")\n",
        "        return\n",
        "\n",
        "    print(\"🚀 SEC QA System initialized successfully!\")\n",
        "\n",
        "    load_dotenv() # This loads the variables from .env\n",
        "\n",
        "    api_key = os.getenv(\"API_KEY\")\n",
        "    database_url = os.getenv(\"DATABASE_URL\")\n",
        "\n",
        "\n",
        "    # Ingest a few companies (small batch for demo)\n",
        "    print(\"\\n📊 Ingesting sample companies...\")\n",
        "    # companies = [\"AAPL\", \"MSFT\", \"TSLA\"]\n",
        "    companies = [\"AAPL\"]\n",
        "\n",
        "    results = await system.ingest_companies(companies, limit_per_company=5)\n",
        "\n",
        "    for ticker, result in results.items():\n",
        "        if result['errors']:\n",
        "            print(f\"  ⚠️ {ticker}: {len(result['errors'])} errors\")\n",
        "            for error in result['errors']:\n",
        "                print(f\"    - {error}\")\n",
        "        else:\n",
        "            print(f\"  ✅ {ticker}: {result['chunks_processed']} chunks in {result['processing_time']:.1f}s\")\n",
        "\n",
        "    # Demo questions using your routing intelligence\n",
        "    print(\"\\n❓ Demo questions:\")\n",
        "\n",
        "    test_questions = [\n",
        "        \"What are Apple's main risk factors?\",\n",
        "        \"What is the trend in Apple's recent revenue ?\",\n",
        "        # \"Compare Apple and Microsoft's R&D spending\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n🤔 {question}\")\n",
        "        try:\n",
        "            result = await system.ask_question(question, max_sources=5)\n",
        "            print(f\"   📈 Confidence: {result.confidence:.2f}\")\n",
        "            print(f\"   🎯 Routing: {result.routing_info['reasoning']}\")\n",
        "            print(f\"   📚 Sources: ({len(result.sources)}) sources: {[[i+1, r['document_url']] for i, r in enumerate(result.sources)]}\")\n",
        "            print(f\"   🤖 Provider: {result.provider_used}\")\n",
        "            print(f\"   ⏱️ Time: {result.processing_time:.1f}s\")\n",
        "            print(f\"   💬 Answer: {result.answer}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error: {e}\")\n",
        "\n",
        "    # Show system status\n",
        "    print(\"\\n📊 System Status:\")\n",
        "    status = await system.get_status()\n",
        "    for key, value in status.items():\n",
        "        if key != \"features\":\n",
        "            print(f\"   {key}: {value}\")\n",
        "\n",
        "    print(\"\\n✅ Demo complete!\")\n",
        "\n",
        "required_vars = [\"EDGAR_IDENTITY\", \"GEMINI_API_KEY\", \"COCKROACH_DATABASE_URL\"]\n",
        "missing = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "if missing:\n",
        "    print(f\"❌ Missing required environment variables: {missing}\")\n",
        "    print(\"Set them like:\")\n",
        "    for var in missing:\n",
        "        if var == \"EDGAR_IDENTITY\":\n",
        "            print(f'export {var}=\"YourName your.email@domain.com\"')\n",
        "        else:\n",
        "            print(f'export {var}=\"your_api_key_here\"')\n",
        "    sys.exit(1)\n",
        "\n",
        "await demo_system()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "aCjasaeuas9z",
        "outputId": "394979ed-72ca-4a9d-db5c-cafaf5e1f869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 SEC QA System initialized successfully!\n",
            "\n",
            "📊 Ingesting sample companies...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✅ AAPL: 84 chunks in 28.1s\n",
            "\n",
            "❓ Demo questions:\n",
            "\n",
            "🤔 What are Apple's main risk factors?\n",
            "   📈 Confidence: 0.97\n",
            "   🎯 Routing: Analysis focused on: AAPL. Temporal strategy: Prioritize most recent filings. Primary sections: 8-K ITEM 8.01, 10-Q ITEM 1A, 10-K ITEM 1A\n",
            "   📚 Sources: (5) sources: [[1, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000073/aapl-20250628.htm'], [2, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000057/aapl-20250329.htm'], [3, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000057/aapl-20250329.htm'], [4, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000114036125027340/ef20052355_8k.htm'], [5, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000114036125027340/ef20052355_8k.htm']]\n",
            "   🤖 Provider: gemini\n",
            "   ⏱️ Time: 15.7s\n",
            "   💬 Answer: The provided context indicates that a comprehensive discussion of Apple's risk factors is located in Part I, Item 1A of the Annual Report on Form 10-K for the fiscal year ended September 28, 2024 (the \"2024 Form 10-K\") and Part II, Item 1A of the Quarterly Report on Form 10-Q [1, Part I, Item 2; 2, Part I, Item 2]. Since these specific sections containing the full list of risk factors are not fully provided in the context, a complete list cannot be given.\n",
            "\n",
            "However, based on the \"Management’s Discussion and Analysis of Financial Condition and Results of Operations\" sections of the FY2025 Q3 and Q2 Form 10-Q filings, the following factors are highlighted as having directly and indirectly impacted, or potentially materially impacting, the Company’s results of operations and financial condition:\n",
            "\n",
            "*   **Macroeconomic Conditions:** These include inflation, interest rates, and currency fluctuations [1, Part I, Item 2, Macroeconomic Conditions; 2, Part I, Item 2, Macroeconomic Conditions].\n",
            "*   **Tariffs and Other Measures:** This encompasses new U.S. Tariffs, additional tariffs on imports from various countries (such as China, India, Japan, South Korea, Taiwan, Vietnam, and the European Union), and reciprocal tariffs or retaliatory measures imposed by other countries. These measures can materially adversely affect Apple's supply chain, the availability of rare earths and other raw materials and components, pricing, and gross margin. Furthermore, trade and other international disputes can negatively impact the overall macroeconomic environment, potentially leading to shifts and reductions in consumer spending and negative consumer sentiment for Apple's products and services [1, Part I, Item 2, Tariffs and Other Measures; 2, Part I, Item 2, Tariffs and Other Trade Measures].\n",
            "*   **Business Seasonality and Product Introductions:** The Company historically experiences higher net sales in its first fiscal quarter due to seasonal holiday demand. The timing of new product and service introductions can significantly impact net sales, cost of sales, and operating expenses. Net sales can also be affected when consumers and distributors anticipate a new product launch [1, Part I, Item 2, Business Seasonality and Product Introductions; 2, Part I, Item 2, Business Seasonality and Product Introductions]....\n",
            "\n",
            "🤔 What is the trend in Apple's recent revenue ?\n",
            "   📈 Confidence: 0.97\n",
            "   🎯 Routing: Analysis focused on: AAPL. Temporal strategy: Analyze across multiple periods for trend analysis. Primary sections: 8-K ITEM 2.02, 10-Q ITEM 2, 10-Q ITEM 1\n",
            "   📚 Sources: (5) sources: [[1, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000073/aapl-20250628.htm'], [2, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000057/aapl-20250329.htm'], [3, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000057/aapl-20250329.htm'], [4, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000032019325000071/aapl-20250731.htm'], [5, 'https://www.sec.gov/ix?doc=/Archives/edgar/data/320193/000114036125027340/ef20052355_8k.htm']]\n",
            "   🤖 Provider: gemini\n",
            "   ⏱️ Time: 17.2s\n",
            "   💬 Answer: Apple's recent revenue shows an increasing trend.\n",
            "\n",
            "For the three months ended June 28, 2025 (Q3 FY2025), Apple reported total net sales of **$94,036 million**, an increase of 10% compared to **$85,777 million** for the same period in 2024 [1, p. 1, 14]. This increase was primarily driven by higher net sales of Products, which rose to $66,613 million from $61,564 million, and Services, which increased to $27,423 million from $24,213 million [1, p. 1]. Specifically, iPhone net sales increased by 13% to $44,582 million, Mac net sales increased by 15% to $8,046 million, and Services net sales increased by 13% to $27,423 million [1, p. 15].\n",
            "\n",
            "For the first nine months of fiscal year 2025 (ending June 28, 2025), total net sales were **$313,695 million**, up 6% from **$296,105 million** in the first nine months of fiscal year 2024 [1, p. 1, 14]. Products sales for this period increased to $233,287 million from $224,908 million, and Services sales increased to $80,408 million from $71,197 million [1, p. 1].\n",
            "\n",
            "This positive trend is also evident in the prior quarter. For the three months ended March 29, 2025 (Q2 FY2025), total net sales increased by 5% to **$95,359 million** from **$90,753 million** in Q2 FY2024 [3, p. 1, 2, p. 13]. Product sales for Q2 FY2025 were $68,714 million (up from $66,886 million) and Services sales were $26,645 million (up from $23,867 million) [3, p. 1].\n",
            "\n",
            "For the first six months of fiscal year 2025 (ending March 29, 2025), total net sales were **$219,659 million**, an increase of 4% from **$210,328 million** in the first six months of fiscal year 2024 [3, p. 1, 2, p. 13]....\n",
            "\n",
            "📊 System Status:\n",
            "   status: healthy\n",
            "   database_provider: cockroach\n",
            "   chunks_stored: 2229\n",
            "   companies_indexed: 4\n",
            "   cache_stats: {'ticker_cache_size': 0, 'embedding_cache_size': 82, 'query_cache_size': 3}\n",
            "\n",
            "✅ Demo complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5-8GxbsaSKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}